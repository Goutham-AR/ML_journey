{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c9f853a1ae7ebd798555968db7afe8d1da83bc5366e8c97fd53fd2ad96c40ed9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, y = np.loadtxt(\"../datasets/3_vars.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X = np.column_stack((np.ones(x1.size), x1, x2, x3))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "Y = y.reshape(-1, 1)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "w = np.zeros((X.shape[1], 1))\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return np.matmul(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(f\"Iteration {i} => Loss: {loss(X, Y, w)}\")\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "eration 99535 => Loss: 6.698178189382944\n",
      "Iteration 99536 => Loss: 6.698178189341272\n",
      "Iteration 99537 => Loss: 6.698178189299602\n",
      "Iteration 99538 => Loss: 6.698178189257934\n",
      "Iteration 99539 => Loss: 6.698178189216282\n",
      "Iteration 99540 => Loss: 6.6981781891746355\n",
      "Iteration 99541 => Loss: 6.698178189132995\n",
      "Iteration 99542 => Loss: 6.698178189091354\n",
      "Iteration 99543 => Loss: 6.698178189049723\n",
      "Iteration 99544 => Loss: 6.698178189008092\n",
      "Iteration 99545 => Loss: 6.698178188966472\n",
      "Iteration 99546 => Loss: 6.698178188924857\n",
      "Iteration 99547 => Loss: 6.69817818888325\n",
      "Iteration 99548 => Loss: 6.698178188841644\n",
      "Iteration 99549 => Loss: 6.6981781888000445\n",
      "Iteration 99550 => Loss: 6.69817818875845\n",
      "Iteration 99551 => Loss: 6.6981781887168665\n",
      "Iteration 99552 => Loss: 6.698178188675281\n",
      "Iteration 99553 => Loss: 6.698178188633704\n",
      "Iteration 99554 => Loss: 6.698178188592132\n",
      "Iteration 99555 => Loss: 6.698178188550571\n",
      "Iteration 99556 => Loss: 6.698178188509013\n",
      "Iteration 99557 => Loss: 6.698178188467455\n",
      "Iteration 99558 => Loss: 6.698178188425911\n",
      "Iteration 99559 => Loss: 6.698178188384361\n",
      "Iteration 99560 => Loss: 6.698178188342824\n",
      "Iteration 99561 => Loss: 6.698178188301297\n",
      "Iteration 99562 => Loss: 6.69817818825977\n",
      "Iteration 99563 => Loss: 6.698178188218247\n",
      "Iteration 99564 => Loss: 6.698178188176735\n",
      "Iteration 99565 => Loss: 6.698178188135221\n",
      "Iteration 99566 => Loss: 6.698178188093714\n",
      "Iteration 99567 => Loss: 6.69817818805222\n",
      "Iteration 99568 => Loss: 6.698178188010726\n",
      "Iteration 99569 => Loss: 6.698178187969245\n",
      "Iteration 99570 => Loss: 6.698178187927763\n",
      "Iteration 99571 => Loss: 6.698178187886285\n",
      "Iteration 99572 => Loss: 6.6981781878448166\n",
      "Iteration 99573 => Loss: 6.698178187803348\n",
      "Iteration 99574 => Loss: 6.6981781877618864\n",
      "Iteration 99575 => Loss: 6.698178187720436\n",
      "Iteration 99576 => Loss: 6.698178187678986\n",
      "Iteration 99577 => Loss: 6.698178187637548\n",
      "Iteration 99578 => Loss: 6.698178187596113\n",
      "Iteration 99579 => Loss: 6.698178187554677\n",
      "Iteration 99580 => Loss: 6.698178187513254\n",
      "Iteration 99581 => Loss: 6.698178187471834\n",
      "Iteration 99582 => Loss: 6.698178187430419\n",
      "Iteration 99583 => Loss: 6.69817818738901\n",
      "Iteration 99584 => Loss: 6.698178187347609\n",
      "Iteration 99585 => Loss: 6.698178187306207\n",
      "Iteration 99586 => Loss: 6.6981781872648165\n",
      "Iteration 99587 => Loss: 6.698178187223432\n",
      "Iteration 99588 => Loss: 6.698178187182048\n",
      "Iteration 99589 => Loss: 6.698178187140676\n",
      "Iteration 99590 => Loss: 6.698178187099308\n",
      "Iteration 99591 => Loss: 6.698178187057945\n",
      "Iteration 99592 => Loss: 6.698178187016586\n",
      "Iteration 99593 => Loss: 6.69817818697523\n",
      "Iteration 99594 => Loss: 6.698178186933878\n",
      "Iteration 99595 => Loss: 6.698178186892544\n",
      "Iteration 99596 => Loss: 6.698178186851206\n",
      "Iteration 99597 => Loss: 6.698178186809873\n",
      "Iteration 99598 => Loss: 6.698178186768547\n",
      "Iteration 99599 => Loss: 6.698178186727234\n",
      "Iteration 99600 => Loss: 6.6981781866859125\n",
      "Iteration 99601 => Loss: 6.698178186644606\n",
      "Iteration 99602 => Loss: 6.698178186603303\n",
      "Iteration 99603 => Loss: 6.698178186562009\n",
      "Iteration 99604 => Loss: 6.698178186520712\n",
      "Iteration 99605 => Loss: 6.6981781864794305\n",
      "Iteration 99606 => Loss: 6.6981781864381515\n",
      "Iteration 99607 => Loss: 6.698178186396874\n",
      "Iteration 99608 => Loss: 6.6981781863556025\n",
      "Iteration 99609 => Loss: 6.698178186314341\n",
      "Iteration 99610 => Loss: 6.698178186273078\n",
      "Iteration 99611 => Loss: 6.698178186231824\n",
      "Iteration 99612 => Loss: 6.698178186190578\n",
      "Iteration 99613 => Loss: 6.698178186149339\n",
      "Iteration 99614 => Loss: 6.698178186108105\n",
      "Iteration 99615 => Loss: 6.698178186066874\n",
      "Iteration 99616 => Loss: 6.698178186025649\n",
      "Iteration 99617 => Loss: 6.698178185984426\n",
      "Iteration 99618 => Loss: 6.6981781859432115\n",
      "Iteration 99619 => Loss: 6.698178185902009\n",
      "Iteration 99620 => Loss: 6.698178185860806\n",
      "Iteration 99621 => Loss: 6.698178185819603\n",
      "Iteration 99622 => Loss: 6.698178185778415\n",
      "Iteration 99623 => Loss: 6.698178185737231\n",
      "Iteration 99624 => Loss: 6.6981781856960465\n",
      "Iteration 99625 => Loss: 6.698178185654876\n",
      "Iteration 99626 => Loss: 6.698178185613702\n",
      "Iteration 99627 => Loss: 6.698178185572539\n",
      "Iteration 99628 => Loss: 6.69817818553138\n",
      "Iteration 99629 => Loss: 6.69817818549023\n",
      "Iteration 99630 => Loss: 6.69817818544908\n",
      "Iteration 99631 => Loss: 6.698178185407938\n",
      "Iteration 99632 => Loss: 6.6981781853668005\n",
      "Iteration 99633 => Loss: 6.69817818532567\n",
      "Iteration 99634 => Loss: 6.698178185284543\n",
      "Iteration 99635 => Loss: 6.6981781852434255\n",
      "Iteration 99636 => Loss: 6.698178185202309\n",
      "Iteration 99637 => Loss: 6.698178185161202\n",
      "Iteration 99638 => Loss: 6.698178185120099\n",
      "Iteration 99639 => Loss: 6.698178185079003\n",
      "Iteration 99640 => Loss: 6.698178185037909\n",
      "Iteration 99641 => Loss: 6.698178184996826\n",
      "Iteration 99642 => Loss: 6.698178184955746\n",
      "Iteration 99643 => Loss: 6.698178184914673\n",
      "Iteration 99644 => Loss: 6.698178184873601\n",
      "Iteration 99645 => Loss: 6.698178184832531\n",
      "Iteration 99646 => Loss: 6.698178184791473\n",
      "Iteration 99647 => Loss: 6.698178184750422\n",
      "Iteration 99648 => Loss: 6.698178184709372\n",
      "Iteration 99649 => Loss: 6.69817818466833\n",
      "Iteration 99650 => Loss: 6.698178184627296\n",
      "Iteration 99651 => Loss: 6.698178184586264\n",
      "Iteration 99652 => Loss: 6.698178184545238\n",
      "Iteration 99653 => Loss: 6.6981781845042185\n",
      "Iteration 99654 => Loss: 6.698178184463205\n",
      "Iteration 99655 => Loss: 6.698178184422196\n",
      "Iteration 99656 => Loss: 6.698178184381191\n",
      "Iteration 99657 => Loss: 6.698178184340194\n",
      "Iteration 99658 => Loss: 6.698178184299201\n",
      "Iteration 99659 => Loss: 6.698178184258215\n",
      "Iteration 99660 => Loss: 6.698178184217233\n",
      "Iteration 99661 => Loss: 6.698178184176258\n",
      "Iteration 99662 => Loss: 6.698178184135288\n",
      "Iteration 99663 => Loss: 6.6981781840943215\n",
      "Iteration 99664 => Loss: 6.698178184053362\n",
      "Iteration 99665 => Loss: 6.698178184012409\n",
      "Iteration 99666 => Loss: 6.69817818397146\n",
      "Iteration 99667 => Loss: 6.698178183930525\n",
      "Iteration 99668 => Loss: 6.69817818388958\n",
      "Iteration 99669 => Loss: 6.698178183848652\n",
      "Iteration 99670 => Loss: 6.698178183807728\n",
      "Iteration 99671 => Loss: 6.698178183766806\n",
      "Iteration 99672 => Loss: 6.698178183725893\n",
      "Iteration 99673 => Loss: 6.698178183684978\n",
      "Iteration 99674 => Loss: 6.698178183644076\n",
      "Iteration 99675 => Loss: 6.698178183603178\n",
      "Iteration 99676 => Loss: 6.698178183562287\n",
      "Iteration 99677 => Loss: 6.698178183521401\n",
      "Iteration 99678 => Loss: 6.698178183480511\n",
      "Iteration 99679 => Loss: 6.698178183439639\n",
      "Iteration 99680 => Loss: 6.69817818339877\n",
      "Iteration 99681 => Loss: 6.698178183357899\n",
      "Iteration 99682 => Loss: 6.698178183317044\n",
      "Iteration 99683 => Loss: 6.698178183276192\n",
      "Iteration 99684 => Loss: 6.698178183235336\n",
      "Iteration 99685 => Loss: 6.698178183194498\n",
      "Iteration 99686 => Loss: 6.698178183153659\n",
      "Iteration 99687 => Loss: 6.698178183112824\n",
      "Iteration 99688 => Loss: 6.698178183072001\n",
      "Iteration 99689 => Loss: 6.698178183031174\n",
      "Iteration 99690 => Loss: 6.6981781829903575\n",
      "Iteration 99691 => Loss: 6.698178182949548\n",
      "Iteration 99692 => Loss: 6.698178182908743\n",
      "Iteration 99693 => Loss: 6.698178182867945\n",
      "Iteration 99694 => Loss: 6.698178182827153\n",
      "Iteration 99695 => Loss: 6.698178182786361\n",
      "Iteration 99696 => Loss: 6.698178182745581\n",
      "Iteration 99697 => Loss: 6.698178182704798\n",
      "Iteration 99698 => Loss: 6.698178182664029\n",
      "Iteration 99699 => Loss: 6.698178182623268\n",
      "Iteration 99700 => Loss: 6.698178182582506\n",
      "Iteration 99701 => Loss: 6.698178182541744\n",
      "Iteration 99702 => Loss: 6.698178182500998\n",
      "Iteration 99703 => Loss: 6.698178182460251\n",
      "Iteration 99704 => Loss: 6.698178182419515\n",
      "Iteration 99705 => Loss: 6.698178182378779\n",
      "Iteration 99706 => Loss: 6.698178182338051\n",
      "Iteration 99707 => Loss: 6.6981781822973305\n",
      "Iteration 99708 => Loss: 6.698178182256608\n",
      "Iteration 99709 => Loss: 6.698178182215902\n",
      "Iteration 99710 => Loss: 6.698178182175196\n",
      "Iteration 99711 => Loss: 6.698178182134495\n",
      "Iteration 99712 => Loss: 6.698178182093801\n",
      "Iteration 99713 => Loss: 6.698178182053111\n",
      "Iteration 99714 => Loss: 6.698178182012424\n",
      "Iteration 99715 => Loss: 6.6981781819717465\n",
      "Iteration 99716 => Loss: 6.698178181931069\n",
      "Iteration 99717 => Loss: 6.698178181890407\n",
      "Iteration 99718 => Loss: 6.698178181849745\n",
      "Iteration 99719 => Loss: 6.698178181809089\n",
      "Iteration 99720 => Loss: 6.698178181768437\n",
      "Iteration 99721 => Loss: 6.6981781817277914\n",
      "Iteration 99722 => Loss: 6.698178181687148\n",
      "Iteration 99723 => Loss: 6.698178181646514\n",
      "Iteration 99724 => Loss: 6.698178181605884\n",
      "Iteration 99725 => Loss: 6.698178181565259\n",
      "Iteration 99726 => Loss: 6.698178181524639\n",
      "Iteration 99727 => Loss: 6.69817818148403\n",
      "Iteration 99728 => Loss: 6.69817818144342\n",
      "Iteration 99729 => Loss: 6.698178181402821\n",
      "Iteration 99730 => Loss: 6.698178181362226\n",
      "Iteration 99731 => Loss: 6.6981781813216354\n",
      "Iteration 99732 => Loss: 6.698178181281047\n",
      "Iteration 99733 => Loss: 6.698178181240462\n",
      "Iteration 99734 => Loss: 6.698178181199892\n",
      "Iteration 99735 => Loss: 6.698178181159324\n",
      "Iteration 99736 => Loss: 6.69817818111876\n",
      "Iteration 99737 => Loss: 6.698178181078196\n",
      "Iteration 99738 => Loss: 6.698178181037648\n",
      "Iteration 99739 => Loss: 6.698178180997103\n",
      "Iteration 99740 => Loss: 6.698178180956559\n",
      "Iteration 99741 => Loss: 6.698178180916021\n",
      "Iteration 99742 => Loss: 6.698178180875485\n",
      "Iteration 99743 => Loss: 6.69817818083497\n",
      "Iteration 99744 => Loss: 6.698178180794448\n",
      "Iteration 99745 => Loss: 6.698178180753928\n",
      "Iteration 99746 => Loss: 6.698178180713422\n",
      "Iteration 99747 => Loss: 6.698178180672926\n",
      "Iteration 99748 => Loss: 6.698178180632422\n",
      "Iteration 99749 => Loss: 6.6981781805919205\n",
      "Iteration 99750 => Loss: 6.69817818055144\n",
      "Iteration 99751 => Loss: 6.698178180510955\n",
      "Iteration 99752 => Loss: 6.698178180470482\n",
      "Iteration 99753 => Loss: 6.698178180430014\n",
      "Iteration 99754 => Loss: 6.698178180389542\n",
      "Iteration 99755 => Loss: 6.698178180349084\n",
      "Iteration 99756 => Loss: 6.698178180308629\n",
      "Iteration 99757 => Loss: 6.698178180268186\n",
      "Iteration 99758 => Loss: 6.698178180227734\n",
      "Iteration 99759 => Loss: 6.698178180187302\n",
      "Iteration 99760 => Loss: 6.69817818014687\n",
      "Iteration 99761 => Loss: 6.69817818010644\n",
      "Iteration 99762 => Loss: 6.69817818006602\n",
      "Iteration 99763 => Loss: 6.698178180025598\n",
      "Iteration 99764 => Loss: 6.698178179985192\n",
      "Iteration 99765 => Loss: 6.6981781799447875\n",
      "Iteration 99766 => Loss: 6.698178179904387\n",
      "Iteration 99767 => Loss: 6.69817817986399\n",
      "Iteration 99768 => Loss: 6.6981781798236\n",
      "Iteration 99769 => Loss: 6.698178179783219\n",
      "Iteration 99770 => Loss: 6.698178179742839\n",
      "Iteration 99771 => Loss: 6.698178179702467\n",
      "Iteration 99772 => Loss: 6.698178179662096\n",
      "Iteration 99773 => Loss: 6.698178179621735\n",
      "Iteration 99774 => Loss: 6.698178179581378\n",
      "Iteration 99775 => Loss: 6.69817817954103\n",
      "Iteration 99776 => Loss: 6.698178179500681\n",
      "Iteration 99777 => Loss: 6.69817817946034\n",
      "Iteration 99778 => Loss: 6.698178179420009\n",
      "Iteration 99779 => Loss: 6.698178179379676\n",
      "Iteration 99780 => Loss: 6.698178179339351\n",
      "Iteration 99781 => Loss: 6.698178179299037\n",
      "Iteration 99782 => Loss: 6.698178179258721\n",
      "Iteration 99783 => Loss: 6.698178179218413\n",
      "Iteration 99784 => Loss: 6.6981781791781145\n",
      "Iteration 99785 => Loss: 6.698178179137819\n",
      "Iteration 99786 => Loss: 6.698178179097522\n",
      "Iteration 99787 => Loss: 6.698178179057239\n",
      "Iteration 99788 => Loss: 6.69817817901696\n",
      "Iteration 99789 => Loss: 6.698178178976682\n",
      "Iteration 99790 => Loss: 6.698178178936413\n",
      "Iteration 99791 => Loss: 6.698178178896151\n",
      "Iteration 99792 => Loss: 6.698178178855893\n",
      "Iteration 99793 => Loss: 6.698178178815639\n",
      "Iteration 99794 => Loss: 6.698178178775389\n",
      "Iteration 99795 => Loss: 6.698178178735145\n",
      "Iteration 99796 => Loss: 6.698178178694911\n",
      "Iteration 99797 => Loss: 6.69817817865468\n",
      "Iteration 99798 => Loss: 6.698178178614453\n",
      "Iteration 99799 => Loss: 6.698178178574236\n",
      "Iteration 99800 => Loss: 6.6981781785340155\n",
      "Iteration 99801 => Loss: 6.698178178493804\n",
      "Iteration 99802 => Loss: 6.698178178453601\n",
      "Iteration 99803 => Loss: 6.698178178413406\n",
      "Iteration 99804 => Loss: 6.698178178373207\n",
      "Iteration 99805 => Loss: 6.698178178333021\n",
      "Iteration 99806 => Loss: 6.698178178292837\n",
      "Iteration 99807 => Loss: 6.698178178252658\n",
      "Iteration 99808 => Loss: 6.6981781782124825\n",
      "Iteration 99809 => Loss: 6.698178178172325\n",
      "Iteration 99810 => Loss: 6.698178178132164\n",
      "Iteration 99811 => Loss: 6.698178178091999\n",
      "Iteration 99812 => Loss: 6.698178178051857\n",
      "Iteration 99813 => Loss: 6.698178178011708\n",
      "Iteration 99814 => Loss: 6.698178177971575\n",
      "Iteration 99815 => Loss: 6.698178177931438\n",
      "Iteration 99816 => Loss: 6.698178177891309\n",
      "Iteration 99817 => Loss: 6.698178177851181\n",
      "Iteration 99818 => Loss: 6.698178177811065\n",
      "Iteration 99819 => Loss: 6.698178177770953\n",
      "Iteration 99820 => Loss: 6.698178177730845\n",
      "Iteration 99821 => Loss: 6.698178177690741\n",
      "Iteration 99822 => Loss: 6.698178177650645\n",
      "Iteration 99823 => Loss: 6.698178177610554\n",
      "Iteration 99824 => Loss: 6.698178177570467\n",
      "Iteration 99825 => Loss: 6.698178177530389\n",
      "Iteration 99826 => Loss: 6.698178177490318\n",
      "Iteration 99827 => Loss: 6.698178177450245\n",
      "Iteration 99828 => Loss: 6.698178177410186\n",
      "Iteration 99829 => Loss: 6.698178177370126\n",
      "Iteration 99830 => Loss: 6.698178177330073\n",
      "Iteration 99831 => Loss: 6.698178177290024\n",
      "Iteration 99832 => Loss: 6.698178177249982\n",
      "Iteration 99833 => Loss: 6.698178177209946\n",
      "Iteration 99834 => Loss: 6.69817817716991\n",
      "Iteration 99835 => Loss: 6.698178177129886\n",
      "Iteration 99836 => Loss: 6.698178177089863\n",
      "Iteration 99837 => Loss: 6.698178177049852\n",
      "Iteration 99838 => Loss: 6.69817817700984\n",
      "Iteration 99839 => Loss: 6.698178176969835\n",
      "Iteration 99840 => Loss: 6.698178176929837\n",
      "Iteration 99841 => Loss: 6.698178176889845\n",
      "Iteration 99842 => Loss: 6.6981781768498525\n",
      "Iteration 99843 => Loss: 6.698178176809873\n",
      "Iteration 99844 => Loss: 6.698178176769896\n",
      "Iteration 99845 => Loss: 6.698178176729922\n",
      "Iteration 99846 => Loss: 6.698178176689955\n",
      "Iteration 99847 => Loss: 6.698178176649993\n",
      "Iteration 99848 => Loss: 6.698178176610037\n",
      "Iteration 99849 => Loss: 6.698178176570085\n",
      "Iteration 99850 => Loss: 6.6981781765301385\n",
      "Iteration 99851 => Loss: 6.698178176490203\n",
      "Iteration 99852 => Loss: 6.698178176450265\n",
      "Iteration 99853 => Loss: 6.698178176410339\n",
      "Iteration 99854 => Loss: 6.698178176370409\n",
      "Iteration 99855 => Loss: 6.698178176330495\n",
      "Iteration 99856 => Loss: 6.698178176290578\n",
      "Iteration 99857 => Loss: 6.698178176250669\n",
      "Iteration 99858 => Loss: 6.698178176210775\n",
      "Iteration 99859 => Loss: 6.698178176170878\n",
      "Iteration 99860 => Loss: 6.698178176130981\n",
      "Iteration 99861 => Loss: 6.698178176091094\n",
      "Iteration 99862 => Loss: 6.6981781760512105\n",
      "Iteration 99863 => Loss: 6.698178176011337\n",
      "Iteration 99864 => Loss: 6.698178175971467\n",
      "Iteration 99865 => Loss: 6.698178175931606\n",
      "Iteration 99866 => Loss: 6.6981781758917505\n",
      "Iteration 99867 => Loss: 6.698178175851893\n",
      "Iteration 99868 => Loss: 6.698178175812042\n",
      "Iteration 99869 => Loss: 6.6981781757722025\n",
      "Iteration 99870 => Loss: 6.6981781757323615\n",
      "Iteration 99871 => Loss: 6.698178175692529\n",
      "Iteration 99872 => Loss: 6.6981781756527\n",
      "Iteration 99873 => Loss: 6.698178175612877\n",
      "Iteration 99874 => Loss: 6.6981781755730685\n",
      "Iteration 99875 => Loss: 6.6981781755332515\n",
      "Iteration 99876 => Loss: 6.698178175493449\n",
      "Iteration 99877 => Loss: 6.6981781754536485\n",
      "Iteration 99878 => Loss: 6.698178175413857\n",
      "Iteration 99879 => Loss: 6.698178175374068\n",
      "Iteration 99880 => Loss: 6.698178175334278\n",
      "Iteration 99881 => Loss: 6.6981781752944975\n",
      "Iteration 99882 => Loss: 6.698178175254725\n",
      "Iteration 99883 => Loss: 6.698178175214963\n",
      "Iteration 99884 => Loss: 6.698178175175193\n",
      "Iteration 99885 => Loss: 6.698178175135439\n",
      "Iteration 99886 => Loss: 6.698178175095689\n",
      "Iteration 99887 => Loss: 6.6981781750559435\n",
      "Iteration 99888 => Loss: 6.698178175016198\n",
      "Iteration 99889 => Loss: 6.698178174976461\n",
      "Iteration 99890 => Loss: 6.698178174936733\n",
      "Iteration 99891 => Loss: 6.698178174897008\n",
      "Iteration 99892 => Loss: 6.698178174857288\n",
      "Iteration 99893 => Loss: 6.698178174817574\n",
      "Iteration 99894 => Loss: 6.698178174777866\n",
      "Iteration 99895 => Loss: 6.69817817473816\n",
      "Iteration 99896 => Loss: 6.698178174698461\n",
      "Iteration 99897 => Loss: 6.698178174658768\n",
      "Iteration 99898 => Loss: 6.698178174619077\n",
      "Iteration 99899 => Loss: 6.698178174579399\n",
      "Iteration 99900 => Loss: 6.698178174539724\n",
      "Iteration 99901 => Loss: 6.698178174500049\n",
      "Iteration 99902 => Loss: 6.698178174460389\n",
      "Iteration 99903 => Loss: 6.698178174420716\n",
      "Iteration 99904 => Loss: 6.698178174381065\n",
      "Iteration 99905 => Loss: 6.698178174341413\n",
      "Iteration 99906 => Loss: 6.698178174301775\n",
      "Iteration 99907 => Loss: 6.6981781742621305\n",
      "Iteration 99908 => Loss: 6.698178174222493\n",
      "Iteration 99909 => Loss: 6.698178174182867\n",
      "Iteration 99910 => Loss: 6.6981781741432425\n",
      "Iteration 99911 => Loss: 6.698178174103622\n",
      "Iteration 99912 => Loss: 6.698178174064009\n",
      "Iteration 99913 => Loss: 6.698178174024403\n",
      "Iteration 99914 => Loss: 6.698178173984805\n",
      "Iteration 99915 => Loss: 6.698178173945207\n",
      "Iteration 99916 => Loss: 6.698178173905609\n",
      "Iteration 99917 => Loss: 6.698178173866028\n",
      "Iteration 99918 => Loss: 6.698178173826443\n",
      "Iteration 99919 => Loss: 6.698178173786875\n",
      "Iteration 99920 => Loss: 6.698178173747298\n",
      "Iteration 99921 => Loss: 6.698178173707735\n",
      "Iteration 99922 => Loss: 6.6981781736681745\n",
      "Iteration 99923 => Loss: 6.698178173628621\n",
      "Iteration 99924 => Loss: 6.698178173589073\n",
      "Iteration 99925 => Loss: 6.698178173549531\n",
      "Iteration 99926 => Loss: 6.698178173509993\n",
      "Iteration 99927 => Loss: 6.698178173470452\n",
      "Iteration 99928 => Loss: 6.698178173430929\n",
      "Iteration 99929 => Loss: 6.698178173391414\n",
      "Iteration 99930 => Loss: 6.698178173351893\n",
      "Iteration 99931 => Loss: 6.698178173312387\n",
      "Iteration 99932 => Loss: 6.698178173272875\n",
      "Iteration 99933 => Loss: 6.6981781732333685\n",
      "Iteration 99934 => Loss: 6.6981781731938685\n",
      "Iteration 99935 => Loss: 6.69817817315439\n",
      "Iteration 99936 => Loss: 6.6981781731149\n",
      "Iteration 99937 => Loss: 6.698178173075418\n",
      "Iteration 99938 => Loss: 6.698178173035944\n",
      "Iteration 99939 => Loss: 6.698178172996478\n",
      "Iteration 99940 => Loss: 6.698178172957017\n",
      "Iteration 99941 => Loss: 6.698178172917557\n",
      "Iteration 99942 => Loss: 6.698178172878104\n",
      "Iteration 99943 => Loss: 6.698178172838657\n",
      "Iteration 99944 => Loss: 6.698178172799218\n",
      "Iteration 99945 => Loss: 6.698178172759773\n",
      "Iteration 99946 => Loss: 6.698178172720344\n",
      "Iteration 99947 => Loss: 6.698178172680918\n",
      "Iteration 99948 => Loss: 6.698178172641494\n",
      "Iteration 99949 => Loss: 6.698178172602076\n",
      "Iteration 99950 => Loss: 6.698178172562668\n",
      "Iteration 99951 => Loss: 6.6981781725232645\n",
      "Iteration 99952 => Loss: 6.698178172483862\n",
      "Iteration 99953 => Loss: 6.698178172444465\n",
      "Iteration 99954 => Loss: 6.698178172405079\n",
      "Iteration 99955 => Loss: 6.698178172365692\n",
      "Iteration 99956 => Loss: 6.698178172326318\n",
      "Iteration 99957 => Loss: 6.698178172286936\n",
      "Iteration 99958 => Loss: 6.698178172247571\n",
      "Iteration 99959 => Loss: 6.698178172208215\n",
      "Iteration 99960 => Loss: 6.698178172168855\n",
      "Iteration 99961 => Loss: 6.698178172129497\n",
      "Iteration 99962 => Loss: 6.6981781720901505\n",
      "Iteration 99963 => Loss: 6.698178172050809\n",
      "Iteration 99964 => Loss: 6.698178172011477\n",
      "Iteration 99965 => Loss: 6.698178171972142\n",
      "Iteration 99966 => Loss: 6.698178171932818\n",
      "Iteration 99967 => Loss: 6.6981781718935\n",
      "Iteration 99968 => Loss: 6.698178171854186\n",
      "Iteration 99969 => Loss: 6.69817817181487\n",
      "Iteration 99970 => Loss: 6.698178171775567\n",
      "Iteration 99971 => Loss: 6.698178171736271\n",
      "Iteration 99972 => Loss: 6.698178171696977\n",
      "Iteration 99973 => Loss: 6.698178171657688\n",
      "Iteration 99974 => Loss: 6.6981781716184035\n",
      "Iteration 99975 => Loss: 6.698178171579125\n",
      "Iteration 99976 => Loss: 6.698178171539851\n",
      "Iteration 99977 => Loss: 6.698178171500586\n",
      "Iteration 99978 => Loss: 6.698178171461325\n",
      "Iteration 99979 => Loss: 6.698178171422068\n",
      "Iteration 99980 => Loss: 6.6981781713828115\n",
      "Iteration 99981 => Loss: 6.698178171343567\n",
      "Iteration 99982 => Loss: 6.698178171304325\n",
      "Iteration 99983 => Loss: 6.698178171265092\n",
      "Iteration 99984 => Loss: 6.698178171225861\n",
      "Iteration 99985 => Loss: 6.698178171186638\n",
      "Iteration 99986 => Loss: 6.69817817114742\n",
      "Iteration 99987 => Loss: 6.698178171108199\n",
      "Iteration 99988 => Loss: 6.6981781710689905\n",
      "Iteration 99989 => Loss: 6.698178171029786\n",
      "Iteration 99990 => Loss: 6.698178170990589\n",
      "Iteration 99991 => Loss: 6.698178170951394\n",
      "Iteration 99992 => Loss: 6.698178170912207\n",
      "Iteration 99993 => Loss: 6.6981781708730255\n",
      "Iteration 99994 => Loss: 6.698178170833847\n",
      "Iteration 99995 => Loss: 6.698178170794671\n",
      "Iteration 99996 => Loss: 6.698178170755507\n",
      "Iteration 99997 => Loss: 6.698178170716345\n",
      "Iteration 99998 => Loss: 6.698178170677191\n",
      "Iteration 99999 => Loss: 6.698178170638038\n"
     ]
    }
   ],
   "source": [
    "w = train(X, Y, iterations=100000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}