{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"294.590391pt\" version=\"1.1\" viewBox=\"0 0 420.365625 294.590391\" width=\"420.365625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-06-21T20:13:50.075141</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 294.590391 \nL 420.365625 294.590391 \nL 420.365625 0 \nL -0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 68.821875 230.338828 \nL 403.621875 230.338828 \nL 403.621875 12.898828 \nL 68.821875 12.898828 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 230.338828 \nL 68.821875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(64.05 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 135.781875 230.338828 \nL 135.781875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(126.238125 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 202.741875 230.338828 \nL 202.741875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(193.198125 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 269.701875 230.338828 \nL 269.701875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g style=\"fill:#262626;\" transform=\"translate(260.158125 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 336.661875 230.338828 \nL 336.661875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(327.118125 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 403.621875 230.338828 \nL 403.621875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(394.078125 251.236484)scale(0.15 -0.15)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Reservations -->\n     <g style=\"fill:#262626;\" transform=\"translate(139.417969 281.151328)scale(0.3 -0.3)\">\n      <defs>\n       <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"240.128906\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"281.242188\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"340.421875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"401.701172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"440.910156\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"468.693359\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"529.875\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"593.253906\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 230.338828 \nL 403.621875 230.338828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(49.778125 236.037656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 186.850828 \nL 403.621875 186.850828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g style=\"fill:#262626;\" transform=\"translate(40.234375 192.549656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 143.362828 \nL 403.621875 143.362828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g style=\"fill:#262626;\" transform=\"translate(40.234375 149.061656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 99.874828 \nL 403.621875 99.874828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 30 -->\n      <g style=\"fill:#262626;\" transform=\"translate(40.234375 105.573656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 56.386828 \nL 403.621875 56.386828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 40 -->\n      <g style=\"fill:#262626;\" transform=\"translate(40.234375 62.085656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#pf690ff36f6)\" d=\"M 68.821875 12.898828 \nL 403.621875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g style=\"fill:#262626;\" transform=\"translate(40.234375 18.597656)scale(0.15 -0.15)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Pizzas -->\n     <g style=\"fill:#262626;\" transform=\"translate(29.995313 167.244609)rotate(-90)scale(0.3 -0.3)\">\n      <defs>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n       <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"58.052734\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"85.835938\" xlink:href=\"#DejaVuSans-122\"/>\n      <use x=\"138.326172\" xlink:href=\"#DejaVuSans-122\"/>\n      <use x=\"190.816406\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"252.095703\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m45b2974860\" style=\"stroke:#4c72b0;\"/>\n    </defs>\n    <g clip-path=\"url(#pf690ff36f6)\">\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"155.869875\" xlink:href=\"#m45b2974860\" y=\"86.828428\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"82.213875\" xlink:href=\"#m45b2974860\" y=\"160.758028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"162.565875\" xlink:href=\"#m45b2974860\" y=\"91.177228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"222.829875\" xlink:href=\"#m45b2974860\" y=\"8.550028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"155.869875\" xlink:href=\"#m45b2974860\" y=\"112.921228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"75.517875\" xlink:href=\"#m45b2974860\" y=\"160.758028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"189.349875\" xlink:href=\"#m45b2974860\" y=\"82.479628\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"135.781875\" xlink:href=\"#m45b2974860\" y=\"156.409228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"242.917875\" xlink:href=\"#m45b2974860\" y=\"104.223628\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"88.909875\" xlink:href=\"#m45b2974860\" y=\"165.106828\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"88.909875\" xlink:href=\"#m45b2974860\" y=\"165.106828\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"209.437875\" xlink:href=\"#m45b2974860\" y=\"91.177228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"115.693875\" xlink:href=\"#m45b2974860\" y=\"134.665228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"216.133875\" xlink:href=\"#m45b2974860\" y=\"69.433228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"82.213875\" xlink:href=\"#m45b2974860\" y=\"173.804428\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"249.613875\" xlink:href=\"#m45b2974860\" y=\"38.991628\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"108.997875\" xlink:href=\"#m45b2974860\" y=\"160.758028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"135.781875\" xlink:href=\"#m45b2974860\" y=\"139.014028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"189.349875\" xlink:href=\"#m45b2974860\" y=\"69.433228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"169.261875\" xlink:href=\"#m45b2974860\" y=\"99.874828\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"129.085875\" xlink:href=\"#m45b2974860\" y=\"117.270028\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"242.917875\" xlink:href=\"#m45b2974860\" y=\"82.479628\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"122.389875\" xlink:href=\"#m45b2974860\" y=\"130.316428\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"169.261875\" xlink:href=\"#m45b2974860\" y=\"60.735628\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"135.781875\" xlink:href=\"#m45b2974860\" y=\"112.921228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"209.437875\" xlink:href=\"#m45b2974860\" y=\"69.433228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"102.301875\" xlink:href=\"#m45b2974860\" y=\"156.409228\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"108.997875\" xlink:href=\"#m45b2974860\" y=\"152.060428\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"155.869875\" xlink:href=\"#m45b2974860\" y=\"121.618828\"/>\n     <use style=\"fill:#4c72b0;stroke:#4c72b0;\" x=\"155.869875\" xlink:href=\"#m45b2974860\" y=\"130.316428\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 68.821875 230.338828 \nL 68.821875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 403.621875 230.338828 \nL 403.621875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 68.821875 230.338828 \nL 403.621875 230.338828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 68.821875 12.898828 \nL 403.621875 12.898828 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf690ff36f6\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"68.821875\" y=\"12.898828\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.axis([0, 50, 0, 50])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Reservations\", fontsize=30)\n",
    "plt.ylabel(\"Pizzas\", fontsize=30)\n",
    "X, Y = np.loadtxt(\"../datasets/data.txt\", skiprows=1, unpack=True)\n",
    "plt.plot(X, Y, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        current_loss = loss(X, Y, w, b)\n",
    "        print(f\"Iteration {i} => loss: {current_loss}\\n\")\n",
    "        \n",
    "        if loss(X, Y, w + lr, b) < current_loss:\n",
    "            w += lr\n",
    "        elif loss(X, Y, w - lr, b) < current_loss:\n",
    "            w -= lr\n",
    "        elif loss(X, Y, w, b + lr) < current_loss:\n",
    "            b += lr\n",
    "        elif loss(X, Y, w, b - lr) < current_loss:\n",
    "            b -= lr\n",
    "        else:\n",
    "            return w, b\n",
    "    raise Exception(\"Couldn't converge within {iterations} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94 => loss: 28.39976666666698\n",
      "\n",
      "Iteration 1095 => loss: 28.376733333333654\n",
      "\n",
      "Iteration 1096 => loss: 28.37528000000036\n",
      "\n",
      "Iteration 1097 => loss: 28.34991333333368\n",
      "\n",
      "Iteration 1098 => loss: 28.324746666667018\n",
      "\n",
      "Iteration 1099 => loss: 28.299780000000336\n",
      "\n",
      "Iteration 1100 => loss: 28.275013333333685\n",
      "\n",
      "Iteration 1101 => loss: 28.25044666666701\n",
      "\n",
      "Iteration 1102 => loss: 28.226080000000344\n",
      "\n",
      "Iteration 1103 => loss: 28.201913333333682\n",
      "\n",
      "Iteration 1104 => loss: 28.177946666667\n",
      "\n",
      "Iteration 1105 => loss: 28.15418000000033\n",
      "\n",
      "Iteration 1106 => loss: 28.130613333333667\n",
      "\n",
      "Iteration 1107 => loss: 28.107246666666985\n",
      "\n",
      "Iteration 1108 => loss: 28.084080000000323\n",
      "\n",
      "Iteration 1109 => loss: 28.06111333333366\n",
      "\n",
      "Iteration 1110 => loss: 28.038346666666985\n",
      "\n",
      "Iteration 1111 => loss: 28.015780000000333\n",
      "\n",
      "Iteration 1112 => loss: 27.993413333333653\n",
      "\n",
      "Iteration 1113 => loss: 27.97124666666698\n",
      "\n",
      "Iteration 1114 => loss: 27.970486666667014\n",
      "\n",
      "Iteration 1115 => loss: 27.945986666667014\n",
      "\n",
      "Iteration 1116 => loss: 27.921686666667025\n",
      "\n",
      "Iteration 1117 => loss: 27.89758666666702\n",
      "\n",
      "Iteration 1118 => loss: 27.873686666667016\n",
      "\n",
      "Iteration 1119 => loss: 27.849986666667007\n",
      "\n",
      "Iteration 1120 => loss: 27.826486666666995\n",
      "\n",
      "Iteration 1121 => loss: 27.803186666667013\n",
      "\n",
      "Iteration 1122 => loss: 27.780086666667\n",
      "\n",
      "Iteration 1123 => loss: 27.757186666667\n",
      "\n",
      "Iteration 1124 => loss: 27.73448666666701\n",
      "\n",
      "Iteration 1125 => loss: 27.711986666667\n",
      "\n",
      "Iteration 1126 => loss: 27.689686666666997\n",
      "\n",
      "Iteration 1127 => loss: 27.66758666666699\n",
      "\n",
      "Iteration 1128 => loss: 27.645686666666982\n",
      "\n",
      "Iteration 1129 => loss: 27.62398666666699\n",
      "\n",
      "Iteration 1130 => loss: 27.602486666666984\n",
      "\n",
      "Iteration 1131 => loss: 27.581186666666976\n",
      "\n",
      "Iteration 1132 => loss: 27.581120000000336\n",
      "\n",
      "Iteration 1133 => loss: 27.55748666666702\n",
      "\n",
      "Iteration 1134 => loss: 27.534053333333677\n",
      "\n",
      "Iteration 1135 => loss: 27.510820000000326\n",
      "\n",
      "Iteration 1136 => loss: 27.487786666666995\n",
      "\n",
      "Iteration 1137 => loss: 27.46495333333366\n",
      "\n",
      "Iteration 1138 => loss: 27.442320000000336\n",
      "\n",
      "Iteration 1139 => loss: 27.419886666666994\n",
      "\n",
      "Iteration 1140 => loss: 27.397653333333654\n",
      "\n",
      "Iteration 1141 => loss: 27.375620000000332\n",
      "\n",
      "Iteration 1142 => loss: 27.353786666666988\n",
      "\n",
      "Iteration 1143 => loss: 27.332153333333647\n",
      "\n",
      "Iteration 1144 => loss: 27.310720000000313\n",
      "\n",
      "Iteration 1145 => loss: 27.28948666666697\n",
      "\n",
      "Iteration 1146 => loss: 27.26845333333365\n",
      "\n",
      "Iteration 1147 => loss: 27.24762000000031\n",
      "\n",
      "Iteration 1148 => loss: 27.22698666666696\n",
      "\n",
      "Iteration 1149 => loss: 27.206553333333652\n",
      "\n",
      "Iteration 1150 => loss: 27.186320000000304\n",
      "\n",
      "Iteration 1151 => loss: 27.184413333333683\n",
      "\n",
      "Iteration 1152 => loss: 27.161846666667007\n",
      "\n",
      "Iteration 1153 => loss: 27.139480000000326\n",
      "\n",
      "Iteration 1154 => loss: 27.117313333333673\n",
      "\n",
      "Iteration 1155 => loss: 27.095346666667\n",
      "\n",
      "Iteration 1156 => loss: 27.073580000000327\n",
      "\n",
      "Iteration 1157 => loss: 27.052013333333647\n",
      "\n",
      "Iteration 1158 => loss: 27.030646666666986\n",
      "\n",
      "Iteration 1159 => loss: 27.00948000000033\n",
      "\n",
      "Iteration 1160 => loss: 26.98851333333365\n",
      "\n",
      "Iteration 1161 => loss: 26.96774666666698\n",
      "\n",
      "Iteration 1162 => loss: 26.947180000000312\n",
      "\n",
      "Iteration 1163 => loss: 26.926813333333648\n",
      "\n",
      "Iteration 1164 => loss: 26.906646666666973\n",
      "\n",
      "Iteration 1165 => loss: 26.8866800000003\n",
      "\n",
      "Iteration 1166 => loss: 26.86691333333363\n",
      "\n",
      "Iteration 1167 => loss: 26.84734666666697\n",
      "\n",
      "Iteration 1168 => loss: 26.8279800000003\n",
      "\n",
      "Iteration 1169 => loss: 26.826766666667\n",
      "\n",
      "Iteration 1170 => loss: 26.805066666666995\n",
      "\n",
      "Iteration 1171 => loss: 26.783566666667\n",
      "\n",
      "Iteration 1172 => loss: 26.762266666667\n",
      "\n",
      "Iteration 1173 => loss: 26.741166666666995\n",
      "\n",
      "Iteration 1174 => loss: 26.720266666666987\n",
      "\n",
      "Iteration 1175 => loss: 26.69956666666699\n",
      "\n",
      "Iteration 1176 => loss: 26.679066666666987\n",
      "\n",
      "Iteration 1177 => loss: 26.65876666666698\n",
      "\n",
      "Iteration 1178 => loss: 26.638666666666975\n",
      "\n",
      "Iteration 1179 => loss: 26.61876666666698\n",
      "\n",
      "Iteration 1180 => loss: 26.599066666666978\n",
      "\n",
      "Iteration 1181 => loss: 26.579566666666974\n",
      "\n",
      "Iteration 1182 => loss: 26.560266666666966\n",
      "\n",
      "Iteration 1183 => loss: 26.541166666666975\n",
      "\n",
      "Iteration 1184 => loss: 26.52226666666697\n",
      "\n",
      "Iteration 1185 => loss: 26.50356666666696\n",
      "\n",
      "Iteration 1186 => loss: 26.485066666666956\n",
      "\n",
      "Iteration 1187 => loss: 26.48454666666698\n",
      "\n",
      "Iteration 1188 => loss: 26.463713333333644\n",
      "\n",
      "Iteration 1189 => loss: 26.443080000000315\n",
      "\n",
      "Iteration 1190 => loss: 26.422646666666978\n",
      "\n",
      "Iteration 1191 => loss: 26.40241333333364\n",
      "\n",
      "Iteration 1192 => loss: 26.382380000000314\n",
      "\n",
      "Iteration 1193 => loss: 26.362546666666972\n",
      "\n",
      "Iteration 1194 => loss: 26.342913333333637\n",
      "\n",
      "Iteration 1195 => loss: 26.323480000000295\n",
      "\n",
      "Iteration 1196 => loss: 26.30424666666695\n",
      "\n",
      "Iteration 1197 => loss: 26.285213333333626\n",
      "\n",
      "Iteration 1198 => loss: 26.266380000000293\n",
      "\n",
      "Iteration 1199 => loss: 26.247746666666956\n",
      "\n",
      "Iteration 1200 => loss: 26.229313333333625\n",
      "\n",
      "Iteration 1201 => loss: 26.211080000000287\n",
      "\n",
      "Iteration 1202 => loss: 26.193046666666948\n",
      "\n",
      "Iteration 1203 => loss: 26.175213333333613\n",
      "\n",
      "Iteration 1204 => loss: 26.157580000000266\n",
      "\n",
      "Iteration 1205 => loss: 26.140146666666944\n",
      "\n",
      "Iteration 1206 => loss: 26.13778666666699\n",
      "\n",
      "Iteration 1207 => loss: 26.118020000000318\n",
      "\n",
      "Iteration 1208 => loss: 26.098453333333637\n",
      "\n",
      "Iteration 1209 => loss: 26.079086666666967\n",
      "\n",
      "Iteration 1210 => loss: 26.059920000000314\n",
      "\n",
      "Iteration 1211 => loss: 26.040953333333622\n",
      "\n",
      "Iteration 1212 => loss: 26.022186666666965\n",
      "\n",
      "Iteration 1213 => loss: 26.003620000000307\n",
      "\n",
      "Iteration 1214 => loss: 25.985253333333635\n",
      "\n",
      "Iteration 1215 => loss: 25.967086666666958\n",
      "\n",
      "Iteration 1216 => loss: 25.94912000000028\n",
      "\n",
      "Iteration 1217 => loss: 25.931353333333615\n",
      "\n",
      "Iteration 1218 => loss: 25.91378666666695\n",
      "\n",
      "Iteration 1219 => loss: 25.896420000000276\n",
      "\n",
      "Iteration 1220 => loss: 25.879253333333608\n",
      "\n",
      "Iteration 1221 => loss: 25.862286666666947\n",
      "\n",
      "Iteration 1222 => loss: 25.845520000000285\n",
      "\n",
      "Iteration 1223 => loss: 25.828953333333608\n",
      "\n",
      "Iteration 1224 => loss: 25.827286666666975\n",
      "\n",
      "Iteration 1225 => loss: 25.808386666666973\n",
      "\n",
      "Iteration 1226 => loss: 25.78968666666697\n",
      "\n",
      "Iteration 1227 => loss: 25.771186666666974\n",
      "\n",
      "Iteration 1228 => loss: 25.75288666666697\n",
      "\n",
      "Iteration 1229 => loss: 25.734786666666952\n",
      "\n",
      "Iteration 1230 => loss: 25.71688666666696\n",
      "\n",
      "Iteration 1231 => loss: 25.69918666666696\n",
      "\n",
      "Iteration 1232 => loss: 25.681686666666952\n",
      "\n",
      "Iteration 1233 => loss: 25.66438666666695\n",
      "\n",
      "Iteration 1234 => loss: 25.647286666666954\n",
      "\n",
      "Iteration 1235 => loss: 25.630386666666947\n",
      "\n",
      "Iteration 1236 => loss: 25.613686666666947\n",
      "\n",
      "Iteration 1237 => loss: 25.597186666666936\n",
      "\n",
      "Iteration 1238 => loss: 25.580886666666935\n",
      "\n",
      "Iteration 1239 => loss: 25.564786666666937\n",
      "\n",
      "Iteration 1240 => loss: 25.548886666666935\n",
      "\n",
      "Iteration 1241 => loss: 25.53318666666693\n",
      "\n",
      "Iteration 1242 => loss: 25.532213333333637\n",
      "\n",
      "Iteration 1243 => loss: 25.514180000000295\n",
      "\n",
      "Iteration 1244 => loss: 25.49634666666697\n",
      "\n",
      "Iteration 1245 => loss: 25.478713333333634\n",
      "\n",
      "Iteration 1246 => loss: 25.461280000000286\n",
      "\n",
      "Iteration 1247 => loss: 25.444046666666964\n",
      "\n",
      "Iteration 1248 => loss: 25.427013333333623\n",
      "\n",
      "Iteration 1249 => loss: 25.410180000000285\n",
      "\n",
      "Iteration 1250 => loss: 25.39354666666695\n",
      "\n",
      "Iteration 1251 => loss: 25.377113333333607\n",
      "\n",
      "Iteration 1252 => loss: 25.360880000000286\n",
      "\n",
      "Iteration 1253 => loss: 25.344846666666943\n",
      "\n",
      "Iteration 1254 => loss: 25.3290133333336\n",
      "\n",
      "Iteration 1255 => loss: 25.313380000000276\n",
      "\n",
      "Iteration 1256 => loss: 25.297946666666938\n",
      "\n",
      "Iteration 1257 => loss: 25.282713333333596\n",
      "\n",
      "Iteration 1258 => loss: 25.26768000000026\n",
      "\n",
      "Iteration 1259 => loss: 25.25284666666692\n",
      "\n",
      "Iteration 1260 => loss: 25.25256666666695\n",
      "\n",
      "Iteration 1261 => loss: 25.235400000000286\n",
      "\n",
      "Iteration 1262 => loss: 25.218433333333618\n",
      "\n",
      "Iteration 1263 => loss: 25.201666666666952\n",
      "\n",
      "Iteration 1264 => loss: 25.18510000000028\n",
      "\n",
      "Iteration 1265 => loss: 25.16873333333361\n",
      "\n",
      "Iteration 1266 => loss: 25.152566666666942\n",
      "\n",
      "Iteration 1267 => loss: 25.13660000000028\n",
      "\n",
      "Iteration 1268 => loss: 25.120833333333596\n",
      "\n",
      "Iteration 1269 => loss: 25.105266666666935\n",
      "\n",
      "Iteration 1270 => loss: 25.089900000000263\n",
      "\n",
      "Iteration 1271 => loss: 25.074733333333594\n",
      "\n",
      "Iteration 1272 => loss: 25.059766666666928\n",
      "\n",
      "Iteration 1273 => loss: 25.045000000000247\n",
      "\n",
      "Iteration 1274 => loss: 25.030433333333587\n",
      "\n",
      "Iteration 1275 => loss: 25.016066666666916\n",
      "\n",
      "Iteration 1276 => loss: 25.001900000000244\n",
      "\n",
      "Iteration 1277 => loss: 24.98793333333357\n",
      "\n",
      "Iteration 1278 => loss: 24.974166666666903\n",
      "\n",
      "Iteration 1279 => loss: 24.972046666666945\n",
      "\n",
      "Iteration 1280 => loss: 24.955946666666946\n",
      "\n",
      "Iteration 1281 => loss: 24.940046666666944\n",
      "\n",
      "Iteration 1282 => loss: 24.92434666666695\n",
      "\n",
      "Iteration 1283 => loss: 24.908846666666935\n",
      "\n",
      "Iteration 1284 => loss: 24.893546666666932\n",
      "\n",
      "Iteration 1285 => loss: 24.878446666666935\n",
      "\n",
      "Iteration 1286 => loss: 24.863546666666938\n",
      "\n",
      "Iteration 1287 => loss: 24.848846666666926\n",
      "\n",
      "Iteration 1288 => loss: 24.834346666666924\n",
      "\n",
      "Iteration 1289 => loss: 24.820046666666922\n",
      "\n",
      "Iteration 1290 => loss: 24.805946666666912\n",
      "\n",
      "Iteration 1291 => loss: 24.792046666666913\n",
      "\n",
      "Iteration 1292 => loss: 24.77834666666691\n",
      "\n",
      "Iteration 1293 => loss: 24.764846666666916\n",
      "\n",
      "Iteration 1294 => loss: 24.751546666666915\n",
      "\n",
      "Iteration 1295 => loss: 24.738446666666906\n",
      "\n",
      "Iteration 1296 => loss: 24.725546666666897\n",
      "\n",
      "Iteration 1297 => loss: 24.724120000000276\n",
      "\n",
      "Iteration 1298 => loss: 24.708886666666942\n",
      "\n",
      "Iteration 1299 => loss: 24.693853333333596\n",
      "\n",
      "Iteration 1300 => loss: 24.679020000000275\n",
      "\n",
      "Iteration 1301 => loss: 24.664386666666932\n",
      "\n",
      "Iteration 1302 => loss: 24.649953333333592\n",
      "\n",
      "Iteration 1303 => loss: 24.63572000000025\n",
      "\n",
      "Iteration 1304 => loss: 24.62168666666691\n",
      "\n",
      "Iteration 1305 => loss: 24.607853333333587\n",
      "\n",
      "Iteration 1306 => loss: 24.59422000000025\n",
      "\n",
      "Iteration 1307 => loss: 24.580786666666903\n",
      "\n",
      "Iteration 1308 => loss: 24.56755333333358\n",
      "\n",
      "Iteration 1309 => loss: 24.554520000000245\n",
      "\n",
      "Iteration 1310 => loss: 24.541686666666894\n",
      "\n",
      "Iteration 1311 => loss: 24.529053333333554\n",
      "\n",
      "Iteration 1312 => loss: 24.516620000000213\n",
      "\n",
      "Iteration 1313 => loss: 24.504386666666896\n",
      "\n",
      "Iteration 1314 => loss: 24.492353333333558\n",
      "\n",
      "Iteration 1315 => loss: 24.49162000000026\n",
      "\n",
      "Iteration 1316 => loss: 24.4772533333336\n",
      "\n",
      "Iteration 1317 => loss: 24.463086666666918\n",
      "\n",
      "Iteration 1318 => loss: 24.44912000000026\n",
      "\n",
      "Iteration 1319 => loss: 24.435353333333587\n",
      "\n",
      "Iteration 1320 => loss: 24.42178666666691\n",
      "\n",
      "Iteration 1321 => loss: 24.40842000000026\n",
      "\n",
      "Iteration 1322 => loss: 24.395253333333585\n",
      "\n",
      "Iteration 1323 => loss: 24.382286666666904\n",
      "\n",
      "Iteration 1324 => loss: 24.36952000000024\n",
      "\n",
      "Iteration 1325 => loss: 24.356953333333557\n",
      "\n",
      "Iteration 1326 => loss: 24.344586666666896\n",
      "\n",
      "Iteration 1327 => loss: 24.33242000000023\n",
      "\n",
      "Iteration 1328 => loss: 24.320453333333553\n",
      "\n",
      "Iteration 1329 => loss: 24.308686666666897\n",
      "\n",
      "Iteration 1330 => loss: 24.297120000000216\n",
      "\n",
      "Iteration 1331 => loss: 24.28575333333355\n",
      "\n",
      "Iteration 1332 => loss: 24.274586666666885\n",
      "\n",
      "Iteration 1333 => loss: 24.274546666666918\n",
      "\n",
      "Iteration 1334 => loss: 24.261046666666918\n",
      "\n",
      "Iteration 1335 => loss: 24.247746666666913\n",
      "\n",
      "Iteration 1336 => loss: 24.23464666666691\n",
      "\n",
      "Iteration 1337 => loss: 24.221746666666903\n",
      "\n",
      "Iteration 1338 => loss: 24.209046666666904\n",
      "\n",
      "Iteration 1339 => loss: 24.1965466666669\n",
      "\n",
      "Iteration 1340 => loss: 24.18424666666689\n",
      "\n",
      "Iteration 1341 => loss: 24.172146666666894\n",
      "\n",
      "Iteration 1342 => loss: 24.16024666666689\n",
      "\n",
      "Iteration 1343 => loss: 24.148546666666892\n",
      "\n",
      "Iteration 1344 => loss: 24.13704666666689\n",
      "\n",
      "Iteration 1345 => loss: 24.125746666666878\n",
      "\n",
      "Iteration 1346 => loss: 24.114646666666875\n",
      "\n",
      "Iteration 1347 => loss: 24.103746666666872\n",
      "\n",
      "Iteration 1348 => loss: 24.09304666666686\n",
      "\n",
      "Iteration 1349 => loss: 24.082546666666868\n",
      "\n",
      "Iteration 1350 => loss: 24.072246666666864\n",
      "\n",
      "Iteration 1351 => loss: 24.062146666666866\n",
      "\n",
      "Iteration 1352 => loss: 24.060266666666905\n",
      "\n",
      "Iteration 1353 => loss: 24.047833333333568\n",
      "\n",
      "Iteration 1354 => loss: 24.035600000000223\n",
      "\n",
      "Iteration 1355 => loss: 24.02356666666689\n",
      "\n",
      "Iteration 1356 => loss: 24.011733333333556\n",
      "\n",
      "Iteration 1357 => loss: 24.00010000000022\n",
      "\n",
      "Iteration 1358 => loss: 23.98866666666688\n",
      "\n",
      "Iteration 1359 => loss: 23.977433333333547\n",
      "\n",
      "Iteration 1360 => loss: 23.96640000000021\n",
      "\n",
      "Iteration 1361 => loss: 23.955566666666872\n",
      "\n",
      "Iteration 1362 => loss: 23.944933333333534\n",
      "\n",
      "Iteration 1363 => loss: 23.934500000000188\n",
      "\n",
      "Iteration 1364 => loss: 23.924266666666867\n",
      "\n",
      "Iteration 1365 => loss: 23.91423333333352\n",
      "\n",
      "Iteration 1366 => loss: 23.904400000000187\n",
      "\n",
      "Iteration 1367 => loss: 23.894766666666857\n",
      "\n",
      "Iteration 1368 => loss: 23.88533333333352\n",
      "\n",
      "Iteration 1369 => loss: 23.87610000000018\n",
      "\n",
      "Iteration 1370 => loss: 23.874913333333556\n",
      "\n",
      "Iteration 1371 => loss: 23.86334666666688\n",
      "\n",
      "Iteration 1372 => loss: 23.851980000000232\n",
      "\n",
      "Iteration 1373 => loss: 23.840813333333553\n",
      "\n",
      "Iteration 1374 => loss: 23.82984666666689\n",
      "\n",
      "Iteration 1375 => loss: 23.81908000000021\n",
      "\n",
      "Iteration 1376 => loss: 23.808513333333533\n",
      "\n",
      "Iteration 1377 => loss: 23.79814666666687\n",
      "\n",
      "Iteration 1378 => loss: 23.787980000000193\n",
      "\n",
      "Iteration 1379 => loss: 23.778013333333526\n",
      "\n",
      "Iteration 1380 => loss: 23.76824666666686\n",
      "\n",
      "Iteration 1381 => loss: 23.758680000000197\n",
      "\n",
      "Iteration 1382 => loss: 23.749313333333525\n",
      "\n",
      "Iteration 1383 => loss: 23.740146666666842\n",
      "\n",
      "Iteration 1384 => loss: 23.73118000000017\n",
      "\n",
      "Iteration 1385 => loss: 23.72241333333351\n",
      "\n",
      "Iteration 1386 => loss: 23.713846666666832\n",
      "\n",
      "Iteration 1387 => loss: 23.705480000000158\n",
      "\n",
      "Iteration 1388 => loss: 23.704986666666887\n",
      "\n",
      "Iteration 1389 => loss: 23.694286666666876\n",
      "\n",
      "Iteration 1390 => loss: 23.68378666666687\n",
      "\n",
      "Iteration 1391 => loss: 23.67348666666686\n",
      "\n",
      "Iteration 1392 => loss: 23.66338666666687\n",
      "\n",
      "Iteration 1393 => loss: 23.65348666666686\n",
      "\n",
      "Iteration 1394 => loss: 23.643786666666852\n",
      "\n",
      "Iteration 1395 => loss: 23.63428666666686\n",
      "\n",
      "Iteration 1396 => loss: 23.624986666666857\n",
      "\n",
      "Iteration 1397 => loss: 23.615886666666846\n",
      "\n",
      "Iteration 1398 => loss: 23.606986666666838\n",
      "\n",
      "Iteration 1399 => loss: 23.59828666666683\n",
      "\n",
      "Iteration 1400 => loss: 23.58978666666684\n",
      "\n",
      "Iteration 1401 => loss: 23.58148666666683\n",
      "\n",
      "Iteration 1402 => loss: 23.573386666666824\n",
      "\n",
      "Iteration 1403 => loss: 23.565486666666832\n",
      "\n",
      "Iteration 1404 => loss: 23.557786666666825\n",
      "\n",
      "Iteration 1405 => loss: 23.55028666666682\n",
      "\n",
      "Iteration 1406 => loss: 23.54298666666681\n",
      "\n",
      "Iteration 1407 => loss: 23.54065333333354\n",
      "\n",
      "Iteration 1408 => loss: 23.5310200000002\n",
      "\n",
      "Iteration 1409 => loss: 23.521586666666856\n",
      "\n",
      "Iteration 1410 => loss: 23.51235333333352\n",
      "\n",
      "Iteration 1411 => loss: 23.503320000000194\n",
      "\n",
      "Iteration 1412 => loss: 23.494486666666845\n",
      "\n",
      "Iteration 1413 => loss: 23.485853333333505\n",
      "\n",
      "Iteration 1414 => loss: 23.477420000000173\n",
      "\n",
      "Iteration 1415 => loss: 23.469186666666836\n",
      "\n",
      "Iteration 1416 => loss: 23.461153333333503\n",
      "\n",
      "Iteration 1417 => loss: 23.453320000000165\n",
      "\n",
      "Iteration 1418 => loss: 23.445686666666823\n",
      "\n",
      "Iteration 1419 => loss: 23.43825333333349\n",
      "\n",
      "Iteration 1420 => loss: 23.431020000000146\n",
      "\n",
      "Iteration 1421 => loss: 23.423986666666806\n",
      "\n",
      "Iteration 1422 => loss: 23.417153333333477\n",
      "\n",
      "Iteration 1423 => loss: 23.410520000000137\n",
      "\n",
      "Iteration 1424 => loss: 23.404086666666803\n",
      "\n",
      "Iteration 1425 => loss: 23.40244666666684\n",
      "\n",
      "Iteration 1426 => loss: 23.393680000000177\n",
      "\n",
      "Iteration 1427 => loss: 23.385113333333507\n",
      "\n",
      "Iteration 1428 => loss: 23.37674666666684\n",
      "\n",
      "Iteration 1429 => loss: 23.368580000000165\n",
      "\n",
      "Iteration 1430 => loss: 23.360613333333497\n",
      "\n",
      "Iteration 1431 => loss: 23.35284666666683\n",
      "\n",
      "Iteration 1432 => loss: 23.34528000000016\n",
      "\n",
      "Iteration 1433 => loss: 23.337913333333482\n",
      "\n",
      "Iteration 1434 => loss: 23.330746666666816\n",
      "\n",
      "Iteration 1435 => loss: 23.323780000000138\n",
      "\n",
      "Iteration 1436 => loss: 23.317013333333474\n",
      "\n",
      "Iteration 1437 => loss: 23.31044666666681\n",
      "\n",
      "Iteration 1438 => loss: 23.304080000000127\n",
      "\n",
      "Iteration 1439 => loss: 23.297913333333465\n",
      "\n",
      "Iteration 1440 => loss: 23.291946666666796\n",
      "\n",
      "Iteration 1441 => loss: 23.286180000000122\n",
      "\n",
      "Iteration 1442 => loss: 23.28061333333345\n",
      "\n",
      "Iteration 1443 => loss: 23.279666666666838\n",
      "\n",
      "Iteration 1444 => loss: 23.271766666666828\n",
      "\n",
      "Iteration 1445 => loss: 23.264066666666825\n",
      "\n",
      "Iteration 1446 => loss: 23.256566666666817\n",
      "\n",
      "Iteration 1447 => loss: 23.249266666666824\n",
      "\n",
      "Iteration 1448 => loss: 23.24216666666681\n",
      "\n",
      "Iteration 1449 => loss: 23.235266666666806\n",
      "\n",
      "Iteration 1450 => loss: 23.228566666666797\n",
      "\n",
      "Iteration 1451 => loss: 23.22206666666681\n",
      "\n",
      "Iteration 1452 => loss: 23.2157666666668\n",
      "\n",
      "Iteration 1453 => loss: 23.209666666666788\n",
      "\n",
      "Iteration 1454 => loss: 23.203766666666795\n",
      "\n",
      "Iteration 1455 => loss: 23.198066666666783\n",
      "\n",
      "Iteration 1456 => loss: 23.192566666666774\n",
      "\n",
      "Iteration 1457 => loss: 23.18726666666677\n",
      "\n",
      "Iteration 1458 => loss: 23.182166666666774\n",
      "\n",
      "Iteration 1459 => loss: 23.177266666666775\n",
      "\n",
      "Iteration 1460 => loss: 23.172566666666764\n",
      "\n",
      "Iteration 1461 => loss: 23.17231333333348\n",
      "\n",
      "Iteration 1462 => loss: 23.16528000000014\n",
      "\n",
      "Iteration 1463 => loss: 23.1584466666668\n",
      "\n",
      "Iteration 1464 => loss: 23.151813333333482\n",
      "\n",
      "Iteration 1465 => loss: 23.145380000000138\n",
      "\n",
      "Iteration 1466 => loss: 23.139146666666793\n",
      "\n",
      "Iteration 1467 => loss: 23.13311333333347\n",
      "\n",
      "Iteration 1468 => loss: 23.127280000000134\n",
      "\n",
      "Iteration 1469 => loss: 23.121646666666784\n",
      "\n",
      "Iteration 1470 => loss: 23.116213333333445\n",
      "\n",
      "Iteration 1471 => loss: 23.110980000000108\n",
      "\n",
      "Iteration 1472 => loss: 23.10594666666678\n",
      "\n",
      "Iteration 1473 => loss: 23.101113333333437\n",
      "\n",
      "Iteration 1474 => loss: 23.096480000000096\n",
      "\n",
      "Iteration 1475 => loss: 23.092046666666768\n",
      "\n",
      "Iteration 1476 => loss: 23.08781333333343\n",
      "\n",
      "Iteration 1477 => loss: 23.08378000000009\n",
      "\n",
      "Iteration 1478 => loss: 23.079946666666743\n",
      "\n",
      "Iteration 1479 => loss: 23.07631333333341\n",
      "\n",
      "Iteration 1480 => loss: 23.07422000000014\n",
      "\n",
      "Iteration 1481 => loss: 23.06825333333346\n",
      "\n",
      "Iteration 1482 => loss: 23.062486666666796\n",
      "\n",
      "Iteration 1483 => loss: 23.05692000000013\n",
      "\n",
      "Iteration 1484 => loss: 23.051553333333445\n",
      "\n",
      "Iteration 1485 => loss: 23.046386666666773\n",
      "\n",
      "Iteration 1486 => loss: 23.04142000000011\n",
      "\n",
      "Iteration 1487 => loss: 23.036653333333444\n",
      "\n",
      "Iteration 1488 => loss: 23.03208666666677\n",
      "\n",
      "Iteration 1489 => loss: 23.027720000000087\n",
      "\n",
      "Iteration 1490 => loss: 23.023553333333428\n",
      "\n",
      "Iteration 1491 => loss: 23.019586666666765\n",
      "\n",
      "Iteration 1492 => loss: 23.015820000000087\n",
      "\n",
      "Iteration 1493 => loss: 23.01225333333341\n",
      "\n",
      "Iteration 1494 => loss: 23.008886666666736\n",
      "\n",
      "Iteration 1495 => loss: 23.005720000000075\n",
      "\n",
      "Iteration 1496 => loss: 23.002753333333406\n",
      "\n",
      "Iteration 1497 => loss: 22.999986666666725\n",
      "\n",
      "Iteration 1498 => loss: 22.99858666666678\n",
      "\n",
      "Iteration 1499 => loss: 22.993486666666783\n",
      "\n",
      "Iteration 1500 => loss: 22.98858666666677\n",
      "\n",
      "Iteration 1501 => loss: 22.98388666666676\n",
      "\n",
      "Iteration 1502 => loss: 22.97938666666676\n",
      "\n",
      "Iteration 1503 => loss: 22.97508666666676\n",
      "\n",
      "Iteration 1504 => loss: 22.970986666666757\n",
      "\n",
      "Iteration 1505 => loss: 22.967086666666752\n",
      "\n",
      "Iteration 1506 => loss: 22.963386666666747\n",
      "\n",
      "Iteration 1507 => loss: 22.95988666666674\n",
      "\n",
      "Iteration 1508 => loss: 22.95658666666673\n",
      "\n",
      "Iteration 1509 => loss: 22.953486666666727\n",
      "\n",
      "Iteration 1510 => loss: 22.950586666666727\n",
      "\n",
      "Iteration 1511 => loss: 22.94788666666672\n",
      "\n",
      "Iteration 1512 => loss: 22.94538666666672\n",
      "\n",
      "Iteration 1513 => loss: 22.943086666666712\n",
      "\n",
      "Iteration 1514 => loss: 22.940986666666713\n",
      "\n",
      "Iteration 1515 => loss: 22.939086666666707\n",
      "\n",
      "Iteration 1516 => loss: 22.938380000000098\n",
      "\n",
      "Iteration 1517 => loss: 22.934146666666756\n",
      "\n",
      "Iteration 1518 => loss: 22.93011333333342\n",
      "\n",
      "Iteration 1519 => loss: 22.926280000000087\n",
      "\n",
      "Iteration 1520 => loss: 22.922646666666743\n",
      "\n",
      "Iteration 1521 => loss: 22.9192133333334\n",
      "\n",
      "Iteration 1522 => loss: 22.915980000000065\n",
      "\n",
      "Iteration 1523 => loss: 22.912946666666738\n",
      "\n",
      "Iteration 1524 => loss: 22.910113333333396\n",
      "\n",
      "Iteration 1525 => loss: 22.907480000000053\n",
      "\n",
      "Iteration 1526 => loss: 22.905046666666728\n",
      "\n",
      "Iteration 1527 => loss: 22.902813333333388\n",
      "\n",
      "Iteration 1528 => loss: 22.90078000000004\n",
      "\n",
      "Iteration 1529 => loss: 22.8989466666667\n",
      "\n",
      "Iteration 1530 => loss: 22.897313333333365\n",
      "\n",
      "Iteration 1531 => loss: 22.895880000000037\n",
      "\n",
      "Iteration 1532 => loss: 22.89464666666669\n",
      "\n",
      "Iteration 1533 => loss: 22.89361333333335\n",
      "\n",
      "Iteration 1534 => loss: 22.893600000000074\n",
      "\n",
      "Iteration 1535 => loss: 22.890233333333402\n",
      "\n",
      "Iteration 1536 => loss: 22.887066666666733\n",
      "\n",
      "Iteration 1537 => loss: 22.88410000000006\n",
      "\n",
      "Iteration 1538 => loss: 22.881333333333387\n",
      "\n",
      "Iteration 1539 => loss: 22.87876666666672\n",
      "\n",
      "Iteration 1540 => loss: 22.876400000000046\n",
      "\n",
      "Iteration 1541 => loss: 22.874233333333386\n",
      "\n",
      "Iteration 1542 => loss: 22.872266666666707\n",
      "\n",
      "Iteration 1543 => loss: 22.870500000000035\n",
      "\n",
      "Iteration 1544 => loss: 22.868933333333363\n",
      "\n",
      "Iteration 1545 => loss: 22.86756666666669\n",
      "\n",
      "Iteration 1546 => loss: 22.866400000000024\n",
      "\n",
      "Iteration 1547 => loss: 22.86543333333335\n",
      "\n",
      "Iteration 1548 => loss: 22.864666666666682\n",
      "\n",
      "Iteration 1549 => loss: 22.86410000000002\n",
      "\n",
      "Iteration 1550 => loss: 22.863733333333336\n",
      "\n",
      "Iteration 1551 => loss: 22.86356666666666\n",
      "\n",
      "1.1000000000000008 12.929999999999769\n"
     ]
    }
   ],
   "source": [
    "w, b = train(X, Y, iterations=1000000, lr=0.01)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34.92999999999978"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "predict(20, w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w, b):\n",
    "    w_gradient = 2 * np.average(X * (predict(X, w, b) - Y))\n",
    "    b_gradient = 2 * np.average(predict(X, w, b) - Y)\n",
    "    return (w_gradient, b_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gd(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        print(f\"Iteration {i}, loss = {loss(X, Y, w, b)}\\n\")\n",
    "        w_gradient, b_gradient = gradient(X, Y, w, b)\n",
    "        w -= w_gradient * lr\n",
    "        b -= b_gradient * lr\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n 19542, loss = 22.842736778071068\n",
      "\n",
      "Iteration 19543, loss = 22.8427367780255\n",
      "\n",
      "Iteration 19544, loss = 22.842736777979997\n",
      "\n",
      "Iteration 19545, loss = 22.842736777934537\n",
      "\n",
      "Iteration 19546, loss = 22.842736777889115\n",
      "\n",
      "Iteration 19547, loss = 22.842736777843758\n",
      "\n",
      "Iteration 19548, loss = 22.842736777798443\n",
      "\n",
      "Iteration 19549, loss = 22.842736777753178\n",
      "\n",
      "Iteration 19550, loss = 22.842736777707945\n",
      "\n",
      "Iteration 19551, loss = 22.842736777662772\n",
      "\n",
      "Iteration 19552, loss = 22.842736777617645\n",
      "\n",
      "Iteration 19553, loss = 22.842736777572565\n",
      "\n",
      "Iteration 19554, loss = 22.842736777527566\n",
      "\n",
      "Iteration 19555, loss = 22.84273677748257\n",
      "\n",
      "Iteration 19556, loss = 22.84273677743763\n",
      "\n",
      "Iteration 19557, loss = 22.84273677739275\n",
      "\n",
      "Iteration 19558, loss = 22.842736777347913\n",
      "\n",
      "Iteration 19559, loss = 22.842736777303127\n",
      "\n",
      "Iteration 19560, loss = 22.842736777258384\n",
      "\n",
      "Iteration 19561, loss = 22.842736777213684\n",
      "\n",
      "Iteration 19562, loss = 22.842736777169037\n",
      "\n",
      "Iteration 19563, loss = 22.842736777124443\n",
      "\n",
      "Iteration 19564, loss = 22.842736777079885\n",
      "\n",
      "Iteration 19565, loss = 22.842736777035377\n",
      "\n",
      "Iteration 19566, loss = 22.842736776990918\n",
      "\n",
      "Iteration 19567, loss = 22.842736776946513\n",
      "\n",
      "Iteration 19568, loss = 22.842736776902154\n",
      "\n",
      "Iteration 19569, loss = 22.842736776857826\n",
      "\n",
      "Iteration 19570, loss = 22.842736776813567\n",
      "\n",
      "Iteration 19571, loss = 22.842736776769353\n",
      "\n",
      "Iteration 19572, loss = 22.842736776725175\n",
      "\n",
      "Iteration 19573, loss = 22.842736776681036\n",
      "\n",
      "Iteration 19574, loss = 22.84273677663696\n",
      "\n",
      "Iteration 19575, loss = 22.842736776592922\n",
      "\n",
      "Iteration 19576, loss = 22.842736776548936\n",
      "\n",
      "Iteration 19577, loss = 22.842736776505003\n",
      "\n",
      "Iteration 19578, loss = 22.842736776461102\n",
      "\n",
      "Iteration 19579, loss = 22.84273677641726\n",
      "\n",
      "Iteration 19580, loss = 22.84273677637346\n",
      "\n",
      "Iteration 19581, loss = 22.842736776329705\n",
      "\n",
      "Iteration 19582, loss = 22.842736776286007\n",
      "\n",
      "Iteration 19583, loss = 22.842736776242326\n",
      "\n",
      "Iteration 19584, loss = 22.842736776198716\n",
      "\n",
      "Iteration 19585, loss = 22.842736776155157\n",
      "\n",
      "Iteration 19586, loss = 22.84273677611163\n",
      "\n",
      "Iteration 19587, loss = 22.84273677606816\n",
      "\n",
      "Iteration 19588, loss = 22.842736776024726\n",
      "\n",
      "Iteration 19589, loss = 22.842736775981333\n",
      "\n",
      "Iteration 19590, loss = 22.842736775938004\n",
      "\n",
      "Iteration 19591, loss = 22.84273677589471\n",
      "\n",
      "Iteration 19592, loss = 22.84273677585147\n",
      "\n",
      "Iteration 19593, loss = 22.84273677580828\n",
      "\n",
      "Iteration 19594, loss = 22.842736775765125\n",
      "\n",
      "Iteration 19595, loss = 22.84273677572202\n",
      "\n",
      "Iteration 19596, loss = 22.842736775678965\n",
      "\n",
      "Iteration 19597, loss = 22.842736775635935\n",
      "\n",
      "Iteration 19598, loss = 22.842736775592968\n",
      "\n",
      "Iteration 19599, loss = 22.842736775550044\n",
      "\n",
      "Iteration 19600, loss = 22.842736775507166\n",
      "\n",
      "Iteration 19601, loss = 22.842736775464328\n",
      "\n",
      "Iteration 19602, loss = 22.842736775421542\n",
      "\n",
      "Iteration 19603, loss = 22.842736775378807\n",
      "\n",
      "Iteration 19604, loss = 22.842736775336107\n",
      "\n",
      "Iteration 19605, loss = 22.842736775293442\n",
      "\n",
      "Iteration 19606, loss = 22.842736775250845\n",
      "\n",
      "Iteration 19607, loss = 22.84273677520829\n",
      "\n",
      "Iteration 19608, loss = 22.842736775165765\n",
      "\n",
      "Iteration 19609, loss = 22.842736775123296\n",
      "\n",
      "Iteration 19610, loss = 22.842736775080873\n",
      "\n",
      "Iteration 19611, loss = 22.842736775038492\n",
      "\n",
      "Iteration 19612, loss = 22.84273677499615\n",
      "\n",
      "Iteration 19613, loss = 22.842736774953867\n",
      "\n",
      "Iteration 19614, loss = 22.842736774911636\n",
      "\n",
      "Iteration 19615, loss = 22.84273677486943\n",
      "\n",
      "Iteration 19616, loss = 22.842736774827284\n",
      "\n",
      "Iteration 19617, loss = 22.842736774785173\n",
      "\n",
      "Iteration 19618, loss = 22.84273677474309\n",
      "\n",
      "Iteration 19619, loss = 22.842736774701077\n",
      "\n",
      "Iteration 19620, loss = 22.8427367746591\n",
      "\n",
      "Iteration 19621, loss = 22.842736774617162\n",
      "\n",
      "Iteration 19622, loss = 22.84273677457529\n",
      "\n",
      "Iteration 19623, loss = 22.84273677453344\n",
      "\n",
      "Iteration 19624, loss = 22.842736774491648\n",
      "\n",
      "Iteration 19625, loss = 22.842736774449897\n",
      "\n",
      "Iteration 19626, loss = 22.84273677440818\n",
      "\n",
      "Iteration 19627, loss = 22.84273677436653\n",
      "\n",
      "Iteration 19628, loss = 22.842736774324905\n",
      "\n",
      "Iteration 19629, loss = 22.842736774283313\n",
      "\n",
      "Iteration 19630, loss = 22.842736774241793\n",
      "\n",
      "Iteration 19631, loss = 22.842736774200297\n",
      "\n",
      "Iteration 19632, loss = 22.842736774158855\n",
      "\n",
      "Iteration 19633, loss = 22.842736774117455\n",
      "\n",
      "Iteration 19634, loss = 22.8427367740761\n",
      "\n",
      "Iteration 19635, loss = 22.842736774034783\n",
      "\n",
      "Iteration 19636, loss = 22.842736773993515\n",
      "\n",
      "Iteration 19637, loss = 22.842736773952296\n",
      "\n",
      "Iteration 19638, loss = 22.84273677391112\n",
      "\n",
      "Iteration 19639, loss = 22.842736773869976\n",
      "\n",
      "Iteration 19640, loss = 22.842736773828893\n",
      "\n",
      "Iteration 19641, loss = 22.84273677378785\n",
      "\n",
      "Iteration 19642, loss = 22.842736773746825\n",
      "\n",
      "Iteration 19643, loss = 22.842736773705884\n",
      "\n",
      "Iteration 19644, loss = 22.84273677366495\n",
      "\n",
      "Iteration 19645, loss = 22.842736773624093\n",
      "\n",
      "Iteration 19646, loss = 22.842736773583244\n",
      "\n",
      "Iteration 19647, loss = 22.842736773542462\n",
      "\n",
      "Iteration 19648, loss = 22.842736773501713\n",
      "\n",
      "Iteration 19649, loss = 22.842736773461024\n",
      "\n",
      "Iteration 19650, loss = 22.84273677342036\n",
      "\n",
      "Iteration 19651, loss = 22.84273677337974\n",
      "\n",
      "Iteration 19652, loss = 22.842736773339173\n",
      "\n",
      "Iteration 19653, loss = 22.842736773298654\n",
      "\n",
      "Iteration 19654, loss = 22.84273677325816\n",
      "\n",
      "Iteration 19655, loss = 22.842736773217723\n",
      "\n",
      "Iteration 19656, loss = 22.84273677317731\n",
      "\n",
      "Iteration 19657, loss = 22.842736773136952\n",
      "\n",
      "Iteration 19658, loss = 22.84273677309665\n",
      "\n",
      "Iteration 19659, loss = 22.842736773056366\n",
      "\n",
      "Iteration 19660, loss = 22.842736773016153\n",
      "\n",
      "Iteration 19661, loss = 22.84273677297597\n",
      "\n",
      "Iteration 19662, loss = 22.84273677293582\n",
      "\n",
      "Iteration 19663, loss = 22.84273677289572\n",
      "\n",
      "Iteration 19664, loss = 22.842736772855663\n",
      "\n",
      "Iteration 19665, loss = 22.842736772815652\n",
      "\n",
      "Iteration 19666, loss = 22.842736772775677\n",
      "\n",
      "Iteration 19667, loss = 22.842736772735748\n",
      "\n",
      "Iteration 19668, loss = 22.842736772695865\n",
      "\n",
      "Iteration 19669, loss = 22.842736772656007\n",
      "\n",
      "Iteration 19670, loss = 22.842736772616213\n",
      "\n",
      "Iteration 19671, loss = 22.842736772576455\n",
      "\n",
      "Iteration 19672, loss = 22.84273677253674\n",
      "\n",
      "Iteration 19673, loss = 22.842736772497066\n",
      "\n",
      "Iteration 19674, loss = 22.842736772457428\n",
      "\n",
      "Iteration 19675, loss = 22.842736772417837\n",
      "\n",
      "Iteration 19676, loss = 22.84273677237828\n",
      "\n",
      "Iteration 19677, loss = 22.842736772338785\n",
      "\n",
      "Iteration 19678, loss = 22.842736772299315\n",
      "\n",
      "Iteration 19679, loss = 22.84273677225989\n",
      "\n",
      "Iteration 19680, loss = 22.842736772220512\n",
      "\n",
      "Iteration 19681, loss = 22.842736772181173\n",
      "\n",
      "Iteration 19682, loss = 22.842736772141883\n",
      "\n",
      "Iteration 19683, loss = 22.842736772102622\n",
      "\n",
      "Iteration 19684, loss = 22.84273677206339\n",
      "\n",
      "Iteration 19685, loss = 22.842736772024224\n",
      "\n",
      "Iteration 19686, loss = 22.8427367719851\n",
      "\n",
      "Iteration 19687, loss = 22.842736771946015\n",
      "\n",
      "Iteration 19688, loss = 22.84273677190696\n",
      "\n",
      "Iteration 19689, loss = 22.842736771867944\n",
      "\n",
      "Iteration 19690, loss = 22.842736771829\n",
      "\n",
      "Iteration 19691, loss = 22.84273677179007\n",
      "\n",
      "Iteration 19692, loss = 22.842736771751202\n",
      "\n",
      "Iteration 19693, loss = 22.842736771712346\n",
      "\n",
      "Iteration 19694, loss = 22.842736771673554\n",
      "\n",
      "Iteration 19695, loss = 22.842736771634797\n",
      "\n",
      "Iteration 19696, loss = 22.84273677159606\n",
      "\n",
      "Iteration 19697, loss = 22.8427367715574\n",
      "\n",
      "Iteration 19698, loss = 22.84273677151877\n",
      "\n",
      "Iteration 19699, loss = 22.84273677148017\n",
      "\n",
      "Iteration 19700, loss = 22.84273677144162\n",
      "\n",
      "Iteration 19701, loss = 22.84273677140311\n",
      "\n",
      "Iteration 19702, loss = 22.84273677136464\n",
      "\n",
      "Iteration 19703, loss = 22.84273677132621\n",
      "\n",
      "Iteration 19704, loss = 22.842736771287818\n",
      "\n",
      "Iteration 19705, loss = 22.84273677124947\n",
      "\n",
      "Iteration 19706, loss = 22.842736771211168\n",
      "\n",
      "Iteration 19707, loss = 22.84273677117291\n",
      "\n",
      "Iteration 19708, loss = 22.842736771134682\n",
      "\n",
      "Iteration 19709, loss = 22.842736771096487\n",
      "\n",
      "Iteration 19710, loss = 22.842736771058334\n",
      "\n",
      "Iteration 19711, loss = 22.842736771020245\n",
      "\n",
      "Iteration 19712, loss = 22.842736770982185\n",
      "\n",
      "Iteration 19713, loss = 22.842736770944157\n",
      "\n",
      "Iteration 19714, loss = 22.842736770906164\n",
      "\n",
      "Iteration 19715, loss = 22.84273677086822\n",
      "\n",
      "Iteration 19716, loss = 22.84273677083032\n",
      "\n",
      "Iteration 19717, loss = 22.842736770792474\n",
      "\n",
      "Iteration 19718, loss = 22.842736770754648\n",
      "\n",
      "Iteration 19719, loss = 22.84273677071686\n",
      "\n",
      "Iteration 19720, loss = 22.842736770679117\n",
      "\n",
      "Iteration 19721, loss = 22.84273677064143\n",
      "\n",
      "Iteration 19722, loss = 22.842736770603757\n",
      "\n",
      "Iteration 19723, loss = 22.842736770566137\n",
      "\n",
      "Iteration 19724, loss = 22.842736770528557\n",
      "\n",
      "Iteration 19725, loss = 22.842736770491022\n",
      "\n",
      "Iteration 19726, loss = 22.842736770453524\n",
      "\n",
      "Iteration 19727, loss = 22.84273677041606\n",
      "\n",
      "Iteration 19728, loss = 22.842736770378636\n",
      "\n",
      "Iteration 19729, loss = 22.842736770341254\n",
      "\n",
      "Iteration 19730, loss = 22.84273677030391\n",
      "\n",
      "Iteration 19731, loss = 22.842736770266605\n",
      "\n",
      "Iteration 19732, loss = 22.842736770229347\n",
      "\n",
      "Iteration 19733, loss = 22.842736770192126\n",
      "\n",
      "Iteration 19734, loss = 22.842736770154943\n",
      "\n",
      "Iteration 19735, loss = 22.8427367701178\n",
      "\n",
      "Iteration 19736, loss = 22.8427367700807\n",
      "\n",
      "Iteration 19737, loss = 22.842736770043633\n",
      "\n",
      "Iteration 19738, loss = 22.8427367700066\n",
      "\n",
      "Iteration 19739, loss = 22.84273676996962\n",
      "\n",
      "Iteration 19740, loss = 22.842736769932678\n",
      "\n",
      "Iteration 19741, loss = 22.842736769895758\n",
      "\n",
      "Iteration 19742, loss = 22.84273676985889\n",
      "\n",
      "Iteration 19743, loss = 22.842736769822075\n",
      "\n",
      "Iteration 19744, loss = 22.842736769785272\n",
      "\n",
      "Iteration 19745, loss = 22.84273676974853\n",
      "\n",
      "Iteration 19746, loss = 22.842736769711816\n",
      "\n",
      "Iteration 19747, loss = 22.84273676967514\n",
      "\n",
      "Iteration 19748, loss = 22.842736769638513\n",
      "\n",
      "Iteration 19749, loss = 22.842736769601917\n",
      "\n",
      "Iteration 19750, loss = 22.84273676956536\n",
      "\n",
      "Iteration 19751, loss = 22.842736769528848\n",
      "\n",
      "Iteration 19752, loss = 22.84273676949236\n",
      "\n",
      "Iteration 19753, loss = 22.842736769455925\n",
      "\n",
      "Iteration 19754, loss = 22.84273676941952\n",
      "\n",
      "Iteration 19755, loss = 22.842736769383166\n",
      "\n",
      "Iteration 19756, loss = 22.842736769346836\n",
      "\n",
      "Iteration 19757, loss = 22.842736769310555\n",
      "\n",
      "Iteration 19758, loss = 22.842736769274303\n",
      "\n",
      "Iteration 19759, loss = 22.8427367692381\n",
      "\n",
      "Iteration 19760, loss = 22.84273676920193\n",
      "\n",
      "Iteration 19761, loss = 22.842736769165803\n",
      "\n",
      "Iteration 19762, loss = 22.842736769129704\n",
      "\n",
      "Iteration 19763, loss = 22.84273676909366\n",
      "\n",
      "Iteration 19764, loss = 22.842736769057634\n",
      "\n",
      "Iteration 19765, loss = 22.84273676902166\n",
      "\n",
      "Iteration 19766, loss = 22.84273676898572\n",
      "\n",
      "Iteration 19767, loss = 22.842736768949813\n",
      "\n",
      "Iteration 19768, loss = 22.84273676891397\n",
      "\n",
      "Iteration 19769, loss = 22.84273676887814\n",
      "\n",
      "Iteration 19770, loss = 22.842736768842357\n",
      "\n",
      "Iteration 19771, loss = 22.842736768806606\n",
      "\n",
      "Iteration 19772, loss = 22.8427367687709\n",
      "\n",
      "Iteration 19773, loss = 22.842736768735215\n",
      "\n",
      "Iteration 19774, loss = 22.842736768699577\n",
      "\n",
      "Iteration 19775, loss = 22.842736768663993\n",
      "\n",
      "Iteration 19776, loss = 22.842736768628424\n",
      "\n",
      "Iteration 19777, loss = 22.842736768592903\n",
      "\n",
      "Iteration 19778, loss = 22.84273676855743\n",
      "\n",
      "Iteration 19779, loss = 22.84273676852197\n",
      "\n",
      "Iteration 19780, loss = 22.842736768486564\n",
      "\n",
      "Iteration 19781, loss = 22.84273676845119\n",
      "\n",
      "Iteration 19782, loss = 22.842736768415868\n",
      "\n",
      "Iteration 19783, loss = 22.842736768380576\n",
      "\n",
      "Iteration 19784, loss = 22.84273676834531\n",
      "\n",
      "Iteration 19785, loss = 22.842736768310097\n",
      "\n",
      "Iteration 19786, loss = 22.842736768274918\n",
      "\n",
      "Iteration 19787, loss = 22.84273676823976\n",
      "\n",
      "Iteration 19788, loss = 22.84273676820466\n",
      "\n",
      "Iteration 19789, loss = 22.84273676816959\n",
      "\n",
      "Iteration 19790, loss = 22.84273676813455\n",
      "\n",
      "Iteration 19791, loss = 22.84273676809956\n",
      "\n",
      "Iteration 19792, loss = 22.8427367680646\n",
      "\n",
      "Iteration 19793, loss = 22.842736768029678\n",
      "\n",
      "Iteration 19794, loss = 22.842736767994793\n",
      "\n",
      "Iteration 19795, loss = 22.842736767959945\n",
      "\n",
      "Iteration 19796, loss = 22.842736767925135\n",
      "\n",
      "Iteration 19797, loss = 22.842736767890372\n",
      "\n",
      "Iteration 19798, loss = 22.84273676785562\n",
      "\n",
      "Iteration 19799, loss = 22.842736767820917\n",
      "\n",
      "Iteration 19800, loss = 22.84273676778626\n",
      "\n",
      "Iteration 19801, loss = 22.84273676775164\n",
      "\n",
      "Iteration 19802, loss = 22.842736767717057\n",
      "\n",
      "Iteration 19803, loss = 22.842736767682492\n",
      "\n",
      "Iteration 19804, loss = 22.842736767647985\n",
      "\n",
      "Iteration 19805, loss = 22.8427367676135\n",
      "\n",
      "Iteration 19806, loss = 22.842736767579062\n",
      "\n",
      "Iteration 19807, loss = 22.84273676754467\n",
      "\n",
      "Iteration 19808, loss = 22.84273676751028\n",
      "\n",
      "Iteration 19809, loss = 22.842736767475955\n",
      "\n",
      "Iteration 19810, loss = 22.842736767441657\n",
      "\n",
      "Iteration 19811, loss = 22.842736767407395\n",
      "\n",
      "Iteration 19812, loss = 22.842736767373182\n",
      "\n",
      "Iteration 19813, loss = 22.84273676733898\n",
      "\n",
      "Iteration 19814, loss = 22.842736767304828\n",
      "\n",
      "Iteration 19815, loss = 22.84273676727072\n",
      "\n",
      "Iteration 19816, loss = 22.842736767236648\n",
      "\n",
      "Iteration 19817, loss = 22.842736767202602\n",
      "\n",
      "Iteration 19818, loss = 22.842736767168606\n",
      "\n",
      "Iteration 19819, loss = 22.84273676713464\n",
      "\n",
      "Iteration 19820, loss = 22.842736767100686\n",
      "\n",
      "Iteration 19821, loss = 22.842736767066803\n",
      "\n",
      "Iteration 19822, loss = 22.842736767032928\n",
      "\n",
      "Iteration 19823, loss = 22.84273676699911\n",
      "\n",
      "Iteration 19824, loss = 22.842736766965324\n",
      "\n",
      "Iteration 19825, loss = 22.842736766931576\n",
      "\n",
      "Iteration 19826, loss = 22.842736766897865\n",
      "\n",
      "Iteration 19827, loss = 22.842736766864167\n",
      "\n",
      "Iteration 19828, loss = 22.84273676683052\n",
      "\n",
      "Iteration 19829, loss = 22.84273676679692\n",
      "\n",
      "Iteration 19830, loss = 22.842736766763338\n",
      "\n",
      "Iteration 19831, loss = 22.842736766729804\n",
      "\n",
      "Iteration 19832, loss = 22.84273676669631\n",
      "\n",
      "Iteration 19833, loss = 22.842736766662835\n",
      "\n",
      "Iteration 19834, loss = 22.842736766629407\n",
      "\n",
      "Iteration 19835, loss = 22.84273676659601\n",
      "\n",
      "Iteration 19836, loss = 22.84273676656265\n",
      "\n",
      "Iteration 19837, loss = 22.842736766529335\n",
      "\n",
      "Iteration 19838, loss = 22.842736766496042\n",
      "\n",
      "Iteration 19839, loss = 22.842736766462778\n",
      "\n",
      "Iteration 19840, loss = 22.842736766429564\n",
      "\n",
      "Iteration 19841, loss = 22.842736766396374\n",
      "\n",
      "Iteration 19842, loss = 22.842736766363224\n",
      "\n",
      "Iteration 19843, loss = 22.842736766330106\n",
      "\n",
      "Iteration 19844, loss = 22.842736766297033\n",
      "\n",
      "Iteration 19845, loss = 22.842736766263997\n",
      "\n",
      "Iteration 19846, loss = 22.84273676623099\n",
      "\n",
      "Iteration 19847, loss = 22.842736766198012\n",
      "\n",
      "Iteration 19848, loss = 22.84273676616507\n",
      "\n",
      "Iteration 19849, loss = 22.84273676613218\n",
      "\n",
      "Iteration 19850, loss = 22.8427367660993\n",
      "\n",
      "Iteration 19851, loss = 22.842736766066476\n",
      "\n",
      "Iteration 19852, loss = 22.842736766033674\n",
      "\n",
      "Iteration 19853, loss = 22.8427367660009\n",
      "\n",
      "Iteration 19854, loss = 22.84273676596819\n",
      "\n",
      "Iteration 19855, loss = 22.842736765935495\n",
      "\n",
      "Iteration 19856, loss = 22.84273676590283\n",
      "\n",
      "Iteration 19857, loss = 22.842736765870203\n",
      "\n",
      "Iteration 19858, loss = 22.842736765837625\n",
      "\n",
      "Iteration 19859, loss = 22.84273676580506\n",
      "\n",
      "Iteration 19860, loss = 22.842736765772546\n",
      "\n",
      "Iteration 19861, loss = 22.842736765740053\n",
      "\n",
      "Iteration 19862, loss = 22.84273676570761\n",
      "\n",
      "Iteration 19863, loss = 22.84273676567521\n",
      "\n",
      "Iteration 19864, loss = 22.84273676564282\n",
      "\n",
      "Iteration 19865, loss = 22.842736765610464\n",
      "\n",
      "Iteration 19866, loss = 22.84273676557815\n",
      "\n",
      "Iteration 19867, loss = 22.842736765545872\n",
      "\n",
      "Iteration 19868, loss = 22.842736765513628\n",
      "\n",
      "Iteration 19869, loss = 22.842736765481423\n",
      "\n",
      "Iteration 19870, loss = 22.84273676544925\n",
      "\n",
      "Iteration 19871, loss = 22.8427367654171\n",
      "\n",
      "Iteration 19872, loss = 22.84273676538499\n",
      "\n",
      "Iteration 19873, loss = 22.842736765352925\n",
      "\n",
      "Iteration 19874, loss = 22.842736765320875\n",
      "\n",
      "Iteration 19875, loss = 22.84273676528887\n",
      "\n",
      "Iteration 19876, loss = 22.842736765256905\n",
      "\n",
      "Iteration 19877, loss = 22.842736765224974\n",
      "\n",
      "Iteration 19878, loss = 22.842736765193074\n",
      "\n",
      "Iteration 19879, loss = 22.842736765161195\n",
      "\n",
      "Iteration 19880, loss = 22.84273676512936\n",
      "\n",
      "Iteration 19881, loss = 22.842736765097563\n",
      "\n",
      "Iteration 19882, loss = 22.842736765065794\n",
      "\n",
      "Iteration 19883, loss = 22.842736765034044\n",
      "\n",
      "Iteration 19884, loss = 22.842736765002364\n",
      "\n",
      "Iteration 19885, loss = 22.842736764970685\n",
      "\n",
      "Iteration 19886, loss = 22.84273676493906\n",
      "\n",
      "Iteration 19887, loss = 22.842736764907464\n",
      "\n",
      "Iteration 19888, loss = 22.8427367648759\n",
      "\n",
      "Iteration 19889, loss = 22.842736764844368\n",
      "\n",
      "Iteration 19890, loss = 22.84273676481286\n",
      "\n",
      "Iteration 19891, loss = 22.842736764781385\n",
      "\n",
      "Iteration 19892, loss = 22.84273676474995\n",
      "\n",
      "Iteration 19893, loss = 22.842736764718563\n",
      "\n",
      "Iteration 19894, loss = 22.842736764687206\n",
      "\n",
      "Iteration 19895, loss = 22.842736764655868\n",
      "\n",
      "Iteration 19896, loss = 22.84273676462458\n",
      "\n",
      "Iteration 19897, loss = 22.842736764593308\n",
      "\n",
      "Iteration 19898, loss = 22.842736764562076\n",
      "\n",
      "Iteration 19899, loss = 22.842736764530873\n",
      "\n",
      "Iteration 19900, loss = 22.84273676449971\n",
      "\n",
      "Iteration 19901, loss = 22.842736764468587\n",
      "\n",
      "Iteration 19902, loss = 22.84273676443748\n",
      "\n",
      "Iteration 19903, loss = 22.842736764406418\n",
      "\n",
      "Iteration 19904, loss = 22.84273676437539\n",
      "\n",
      "Iteration 19905, loss = 22.842736764344373\n",
      "\n",
      "Iteration 19906, loss = 22.84273676431341\n",
      "\n",
      "Iteration 19907, loss = 22.84273676428249\n",
      "\n",
      "Iteration 19908, loss = 22.842736764251576\n",
      "\n",
      "Iteration 19909, loss = 22.84273676422071\n",
      "\n",
      "Iteration 19910, loss = 22.84273676418987\n",
      "\n",
      "Iteration 19911, loss = 22.842736764159074\n",
      "\n",
      "Iteration 19912, loss = 22.842736764128304\n",
      "\n",
      "Iteration 19913, loss = 22.842736764097573\n",
      "\n",
      "Iteration 19914, loss = 22.842736764066856\n",
      "\n",
      "Iteration 19915, loss = 22.842736764036186\n",
      "\n",
      "Iteration 19916, loss = 22.842736764005547\n",
      "\n",
      "Iteration 19917, loss = 22.84273676397494\n",
      "\n",
      "Iteration 19918, loss = 22.84273676394437\n",
      "\n",
      "Iteration 19919, loss = 22.84273676391382\n",
      "\n",
      "Iteration 19920, loss = 22.84273676388332\n",
      "\n",
      "Iteration 19921, loss = 22.84273676385283\n",
      "\n",
      "Iteration 19922, loss = 22.842736763822398\n",
      "\n",
      "Iteration 19923, loss = 22.84273676379198\n",
      "\n",
      "Iteration 19924, loss = 22.842736763761604\n",
      "\n",
      "Iteration 19925, loss = 22.842736763731253\n",
      "\n",
      "Iteration 19926, loss = 22.842736763700945\n",
      "\n",
      "Iteration 19927, loss = 22.842736763670658\n",
      "\n",
      "Iteration 19928, loss = 22.842736763640406\n",
      "\n",
      "Iteration 19929, loss = 22.842736763610194\n",
      "\n",
      "Iteration 19930, loss = 22.842736763580003\n",
      "\n",
      "Iteration 19931, loss = 22.842736763549848\n",
      "\n",
      "Iteration 19932, loss = 22.842736763519724\n",
      "\n",
      "Iteration 19933, loss = 22.842736763489633\n",
      "\n",
      "Iteration 19934, loss = 22.842736763459573\n",
      "\n",
      "Iteration 19935, loss = 22.84273676342955\n",
      "\n",
      "Iteration 19936, loss = 22.842736763399554\n",
      "\n",
      "Iteration 19937, loss = 22.842736763369583\n",
      "\n",
      "Iteration 19938, loss = 22.84273676333966\n",
      "\n",
      "Iteration 19939, loss = 22.842736763309755\n",
      "\n",
      "Iteration 19940, loss = 22.8427367632799\n",
      "\n",
      "Iteration 19941, loss = 22.842736763250052\n",
      "\n",
      "Iteration 19942, loss = 22.842736763220255\n",
      "\n",
      "Iteration 19943, loss = 22.84273676319047\n",
      "\n",
      "Iteration 19944, loss = 22.84273676316074\n",
      "\n",
      "Iteration 19945, loss = 22.84273676313103\n",
      "\n",
      "Iteration 19946, loss = 22.842736763101357\n",
      "\n",
      "Iteration 19947, loss = 22.842736763071716\n",
      "\n",
      "Iteration 19948, loss = 22.842736763042094\n",
      "\n",
      "Iteration 19949, loss = 22.84273676301251\n",
      "\n",
      "Iteration 19950, loss = 22.842736762982963\n",
      "\n",
      "Iteration 19951, loss = 22.842736762953436\n",
      "\n",
      "Iteration 19952, loss = 22.84273676292395\n",
      "\n",
      "Iteration 19953, loss = 22.84273676289449\n",
      "\n",
      "Iteration 19954, loss = 22.84273676286507\n",
      "\n",
      "Iteration 19955, loss = 22.842736762835667\n",
      "\n",
      "Iteration 19956, loss = 22.842736762806307\n",
      "\n",
      "Iteration 19957, loss = 22.842736762776983\n",
      "\n",
      "Iteration 19958, loss = 22.842736762747673\n",
      "\n",
      "Iteration 19959, loss = 22.84273676271841\n",
      "\n",
      "Iteration 19960, loss = 22.84273676268916\n",
      "\n",
      "Iteration 19961, loss = 22.842736762659957\n",
      "\n",
      "Iteration 19962, loss = 22.842736762630793\n",
      "\n",
      "Iteration 19963, loss = 22.842736762601636\n",
      "\n",
      "Iteration 19964, loss = 22.842736762572528\n",
      "\n",
      "Iteration 19965, loss = 22.84273676254344\n",
      "\n",
      "Iteration 19966, loss = 22.842736762514395\n",
      "\n",
      "Iteration 19967, loss = 22.84273676248537\n",
      "\n",
      "Iteration 19968, loss = 22.84273676245638\n",
      "\n",
      "Iteration 19969, loss = 22.84273676242741\n",
      "\n",
      "Iteration 19970, loss = 22.842736762398484\n",
      "\n",
      "Iteration 19971, loss = 22.842736762369594\n",
      "\n",
      "Iteration 19972, loss = 22.84273676234072\n",
      "\n",
      "Iteration 19973, loss = 22.84273676231188\n",
      "\n",
      "Iteration 19974, loss = 22.842736762283074\n",
      "\n",
      "Iteration 19975, loss = 22.842736762254287\n",
      "\n",
      "Iteration 19976, loss = 22.842736762225556\n",
      "\n",
      "Iteration 19977, loss = 22.84273676219682\n",
      "\n",
      "Iteration 19978, loss = 22.84273676216815\n",
      "\n",
      "Iteration 19979, loss = 22.84273676213949\n",
      "\n",
      "Iteration 19980, loss = 22.84273676211087\n",
      "\n",
      "Iteration 19981, loss = 22.842736762082275\n",
      "\n",
      "Iteration 19982, loss = 22.842736762053725\n",
      "\n",
      "Iteration 19983, loss = 22.842736762025194\n",
      "\n",
      "Iteration 19984, loss = 22.842736761996687\n",
      "\n",
      "Iteration 19985, loss = 22.842736761968215\n",
      "\n",
      "Iteration 19986, loss = 22.84273676193977\n",
      "\n",
      "Iteration 19987, loss = 22.842736761911357\n",
      "\n",
      "Iteration 19988, loss = 22.84273676188299\n",
      "\n",
      "Iteration 19989, loss = 22.842736761854628\n",
      "\n",
      "Iteration 19990, loss = 22.842736761826302\n",
      "\n",
      "Iteration 19991, loss = 22.842736761798008\n",
      "\n",
      "Iteration 19992, loss = 22.842736761769775\n",
      "\n",
      "Iteration 19993, loss = 22.842736761741527\n",
      "\n",
      "Iteration 19994, loss = 22.84273676171334\n",
      "\n",
      "Iteration 19995, loss = 22.84273676168516\n",
      "\n",
      "Iteration 19996, loss = 22.842736761657026\n",
      "\n",
      "Iteration 19997, loss = 22.842736761628903\n",
      "\n",
      "Iteration 19998, loss = 22.842736761600822\n",
      "\n",
      "Iteration 19999, loss = 22.84273676157277\n",
      "\n",
      "1.0811301699901938 13.172267656369339\n"
     ]
    }
   ],
   "source": [
    "w, b = train_gd(X, Y, iterations=20000, lr=0.001)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34.79487105617322"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "predict(20, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "c9f853a1ae7ebd798555968db7afe8d1da83bc5366e8c97fd53fd2ad96c40ed9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}